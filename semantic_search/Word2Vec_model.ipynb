{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search Word2Vec Model First Draft\n",
    "\n",
    "This Jupyter notebook is meant to serve as an introduction to reading Github `.md` documentation and analyzing it..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Documentation Data Reading and Pre-Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Reading and Storing the Documentation Data\n",
    "\n",
    "In this section, we'll read the markdown `.md` file data, collect it, and store it for processing. We can do this by reading through all of the `.md` files in a directory and reading them into plain text format, then storing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doc_reader as reader\n",
    "\n",
    "doc_data = reader.collect_doc_data(\"docs/docs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Cleaning the Documentation Data\n",
    "\n",
    "In this section, we'll take our collected and stored documentation data from Step 1 and clean it up so we can use it. This could include removing HTML tags, removing punctuation and special characters, removing extra whitespaces from the text, making all of our text lowercase for semantic searching, and catching any mispellings in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import md_cleaner as cleaner\n",
    "\n",
    "cleaned_doc_data = cleaner.clean_doc_data(doc_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Pre-processing the Documentation Data\n",
    "\n",
    "In this section, we'll take our cleaned documentation data from Step 2 and pre-process it by tokenization, stemming lemmatization, and stop-word removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import md_preprocessor as preprocessor\n",
    "\n",
    "preproc_docs = preprocessor.preprocess_doc_data(cleaned_doc_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Implementing Semantic Search with Word2Vec\n",
    "\n",
    "Now, we can use `Gensim` to implement the semantic searching of the cleaned and pre-processed documentation data with its Word2Vec algorithm. This basically maps words and phrases to dense vector representations in a high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "pretrained_model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "corpus = list(preproc_docs.values())\n",
    "model = Word2Vec(corpus, vector_size=500, window=5, min_count=5, workers=4)\n",
    "model.build_vocab_from_freq(pretrained_model.key_to_index, corpus_count=len(corpus), update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = {}\n",
    "\n",
    "for filename, tokens in preproc_docs.items():\n",
    "    embeddings = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if embeddings:\n",
    "        document_embeddings[filename] = gensim.matutils.unitvec(np.mean(embeddings, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query_str: str):\n",
    "    \"\"\"\n",
    "    Performs a similarity search on the inputted query against the given Word2Vec model.\n",
    "\n",
    "    Parameters:\n",
    "        query_str (str) : user query\n",
    "    \n",
    "    Returns:\n",
    "        similar_docs (list[str]) : most similar docs in order\n",
    "    \"\"\"\n",
    "    corrected_query = cleaner.correct_spelling(query_str)\n",
    "    query_tokens = preprocessor.preprocess_str(cleaner.clean_str(corrected_query))\n",
    "    average_vec_rep = [model.wv[token] for token in query_tokens if token in model.wv]\n",
    "    query_embedding = gensim.matutils.unitvec(np.mean(average_vec_rep, axis=0))\n",
    "\n",
    "    similar_docs = []\n",
    "    for filename, doc_embedding in document_embeddings.items():\n",
    "        similarity_score = np.dot(doc_embedding, query_embedding)\n",
    "        similar_docs.append((filename, similarity_score))\n",
    "    \n",
    "    similar_docs = sorted(similar_docs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_files(query, top_k=5, include_score=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Gets the top 'k' relevant files from an inputted query. Defaults to top\n",
    "    5 most relevant files.\n",
    "\n",
    "    Parameters:\n",
    "        query (str) : question to search PW documentation for\n",
    "        top_k (int) : top 'k' most relevant files to return (default: 5)\n",
    "        include_score (bool) : if True, includes similarity score of file\n",
    "        verbose (bool) : if True, prints files in addition to returning\n",
    "    \n",
    "    Returns:\n",
    "        rel_files (list) : top 'k' most relevant files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        similar_docs = run_query(query)\n",
    "    except TypeError:\n",
    "        print(\"Your query does not match anything in our system.\")\n",
    "        return []\n",
    "\n",
    "    if include_score:\n",
    "        rel_files = similar_docs[:top_k]\n",
    "        if verbose:\n",
    "            print(f\"Top {top_k} most relevant files to your query with similarity scores included:\\n\")\n",
    "            for i, (file, sim_score) in enumerate(rel_files):\n",
    "                print(f\"{i + 1}. {file}: {sim_score}\")\n",
    "        return rel_files\n",
    "    else:\n",
    "        rel_files = [filename for filename, _ in similar_docs[:top_k]]\n",
    "        if verbose:\n",
    "            print(f\"Top {top_k} most relevant files to your query:\\n\")\n",
    "            for i, file in enumerate(rel_files):\n",
    "                print(f\"{i + 1}. {file}\")\n",
    "    return rel_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_content_from_filenames(filenames: list, docs: dict) -> list:\n",
    "    \"\"\"\n",
    "    Helper function that takes a list of filenames and returns a list of\n",
    "    the cleaned and preprocessed contents of those files.\n",
    "\n",
    "    Parameters:\n",
    "        filenames (list) : filenames to get content of\n",
    "        docs (dict) : documents keyed by filename and valued with content\n",
    "\n",
    "    Returns:\n",
    "        file_content (list) : content of each of the inputted filenames \n",
    "    \"\"\"\n",
    "    return {file: docs[file] for file in filenames}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_gpt_input(gpt_docs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Helper function to turn the dict list\n",
    "    \"\"\"\n",
    "    new_docs = {}\n",
    "    for file, content in gpt_docs.items():\n",
    "        new_docs[file] = \" \".join(content)\n",
    "    \n",
    "    return new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_token_length(*args):\n",
    "    \"\"\"\n",
    "    Helper function to make sure that inputted tokens are below the 4097\n",
    "    token input cap.\n",
    "\n",
    "    Paramaters:\n",
    "        *args : args\n",
    "    \n",
    "    Returns:\n",
    "        (bool) : if the inputted arguments are below 4097 tokens\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import helper_funcs as helper\n",
    "\n",
    "def run_gpt(query, *args, api_key=helper.get_api_key()):\n",
    "    \"\"\"\n",
    "    Function that runs the gpt-3.5-turbo AI API on a query and set of arguments\n",
    "    Arguments should consist of a variable length list, where each\n",
    "    element contains a list of tokens from the most relevant files related to\n",
    "    the inputted query.\n",
    "\n",
    "    Paramaters:\n",
    "        query (str) : inputted query from user\n",
    "        *args (list[list[str]]) : array containing document information tokens\n",
    "        api_key (str) : user API key to run\n",
    "    \n",
    "    Returns:\n",
    "        reply (str) : GPT AI response to query with supporting relevant documents\n",
    "    \"\"\"\n",
    "    openai.api_key = api_key\n",
    "\n",
    "    gpt_prompt = \"You are a helpful assistant in charge of helping users understand our platform.\"\n",
    "    clarification_1 = \"Your responses should not require users to search through our files. Instead, you can include relevant filenames as additional support resources if they need it.\"\n",
    "    clarification_2 = \"When including a filename in your response, print in Proper formatting -- so without dashes or the file extensions.\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": gpt_prompt},\n",
    "        {\"role\": \"system\", \"content\": clarification_1},\n",
    "        {\"role\": \"system\", \"content\": clarification_2},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "\n",
    "    for tokens in args:\n",
    "        messages.append({\"role\": \"user\", \"content\": json.dumps(tokens)})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    reply = response.choices[0].message.content\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do I create a new user container?\"\n",
    "\n",
    "rel_docs = get_relevant_files(query)\n",
    "\n",
    "rel_content = get_file_content_from_filenames(rel_docs, preproc_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_rel_content = format_gpt_input(rel_content)\n",
    "# response = run_gpt(query, formatted_rel_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT: To create a new user container, navigate to your organization settings, select the \"User\" tab, and click \"Add User.\" On the next page, you can enter the new user's credentials such as their name, username, email, and password. You can also choose to add their location (optional). Once you have completed this form, click \"Create Account\" and the new user will be listed in the User tab.\n",
      "\n",
      "If you need to add several new users, you can use the mass-importing method. Navigate to the organization settings and click the \"User\" tab, then choose \"Import User\". Follow the steps and use the CSV template provided to input the new user's information. Once completed, click \"Import User,\" and the new users will be listed in the User tab. \n",
      "\n",
      "For troubleshooting, please note that sometimes CSV files can cause issues with compatibility. MacOS, for example, sometimes saves CSV files as a \".numbers\" file or prompts to save it as an Excel format file instead of CSV. However, please note that the user import feature only accepts CSV files. If any errors occur, they will be displayed in the import log that appears at the bottom of the screen.\n"
     ]
    }
   ],
   "source": [
    "print(f\"ChatGPT: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
