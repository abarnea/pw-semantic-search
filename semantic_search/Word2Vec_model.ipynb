{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search Word2Vec Model First Draft\n",
    "\n",
    "This Jupyter notebook is meant to serve as an introduction to reading Github `.md` documentation and analyzing it..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Documentation Data Reading and Pre-Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Reading and Storing the Documentation Data\n",
    "\n",
    "In this section, we'll read the markdown `.md` file data, collect it, and store it for processing. We can do this by reading through all of the `.md` files in a directory and reading them into plain text format, then storing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doc_reader as reader\n",
    "\n",
    "doc_data = reader.collect_doc_data(\"docs/docs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Cleaning the Documentation Data\n",
    "\n",
    "In this section, we'll take our collected and stored documentation data from Step 1 and clean it up so we can use it. This could include removing HTML tags, removing punctuation and special characters, removing extra whitespaces from the text, making all of our text lowercase for semantic searching, and catching any mispellings in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import md_cleaner as cleaner\n",
    "\n",
    "cleaned_doc_data = cleaner.clean_doc_data(doc_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Pre-processing the Documentation Data\n",
    "\n",
    "In this section, we'll take our cleaned documentation data from Step 2 and pre-process it by tokenization, stemming lemmatization, and stop-word removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import md_preprocessor as preprocessor\n",
    "\n",
    "preproc_docs = preprocessor.preprocess_doc_data(cleaned_doc_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Implementing Semantic Search with Word2Vec\n",
    "\n",
    "Now, we can use `Gensim` to implement the semantic searching of the cleaned and pre-processed documentation data with its Word2Vec algorithm. This basically maps words and phrases to dense vector representations in a high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "pretrained_model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "corpus = list(preproc_docs.values())\n",
    "model = Word2Vec(corpus, vector_size=500, window=5, min_count=5, workers=4)\n",
    "model.build_vocab_from_freq(pretrained_model.key_to_index, corpus_count=len(corpus), update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = {}\n",
    "\n",
    "for filename, tokens in preproc_docs.items():\n",
    "    embeddings = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if embeddings:\n",
    "        document_embeddings[filename] = gensim.matutils.unitvec(np.mean(embeddings, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query_str: str):\n",
    "    \"\"\"\n",
    "    Performs a similarity search on the inputted query against the given Word2Vec model.\n",
    "\n",
    "    Parameters:\n",
    "        query_str (str) : user query\n",
    "    \n",
    "    Returns:\n",
    "        similar_docs (list[str]) : most similar docs in order\n",
    "    \"\"\"\n",
    "    corrected_query = cleaner.correct_spelling(query_str)\n",
    "    query_tokens = preprocessor.preprocess_str(cleaner.clean_str(corrected_query))\n",
    "    average_vec_rep = [model.wv[token] for token in query_tokens if token in model.wv]\n",
    "    query_embedding = gensim.matutils.unitvec(np.mean(average_vec_rep, axis=0))\n",
    "\n",
    "    similar_docs = []\n",
    "    for filename, doc_embedding in document_embeddings.items():\n",
    "        similarity_score = np.dot(doc_embedding, query_embedding)\n",
    "        similar_docs.append((filename, similarity_score))\n",
    "    \n",
    "    similar_docs = sorted(similar_docs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_files(query, top_k=5, include_score=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Gets the top 'k' relevant files from an inputted query. Defaults to top\n",
    "    5 most relevant files.\n",
    "\n",
    "    Parameters:\n",
    "        query (str) : question to search PW documentation for\n",
    "        top_k (int) : top 'k' most relevant files to return (default: 5)\n",
    "        include_score (bool) : if True, includes similarity score of file\n",
    "        verbose (bool) : if True, prints files in addition to returning\n",
    "    \n",
    "    Returns:\n",
    "        rel_files (list) : top 'k' most relevant files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        similar_docs = run_query(query)\n",
    "    except TypeError:\n",
    "        print(\"Your query does not match anything in our system.\")\n",
    "        return []\n",
    "\n",
    "    if include_score:\n",
    "        rel_files = similar_docs[:top_k]\n",
    "        if verbose:\n",
    "            print(f\"Top {top_k} most relevant files to your query with similarity scores included:\\n\")\n",
    "            for i, (file, sim_score) in enumerate(rel_files):\n",
    "                print(f\"{i + 1}. {file}: {sim_score}\")\n",
    "        return rel_files\n",
    "    else:\n",
    "        rel_files = [filename for filename, _ in similar_docs[:top_k]]\n",
    "        if verbose:\n",
    "            print(f\"Top {top_k} most relevant files to your query:\\n\")\n",
    "            for i, file in enumerate(rel_files):\n",
    "                print(f\"{i + 1}. {file}\")\n",
    "    return rel_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, val in enumerate(preproc_docs.values()):\n",
    "#     print(f\"{i}. {len(val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most relevant files to your query with similarity scores included:\n",
      "\n",
      "1. transferring-data-aws.md: 0.9999222755432129\n",
      "2. creating-storage.md: 0.9999147653579712\n",
      "3. creating-clusters.md: 0.9999105334281921\n",
      "4. configuring-storage.md: 0.9999102354049683\n",
      "5. configuring-clusters.md: 0.9999101161956787\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the best way to authenticate into an S3 bucket?\"\n",
    "\n",
    "get_relevant_files(query, include_score=True, verbose=True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
