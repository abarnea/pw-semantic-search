{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search Word2Vec Model First Draft\n",
    "\n",
    "This Jupyter notebook is meant to serve as an introduction to reading Github `.md` documentation and analyzing it..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Documentation Data Reading and Pre-Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Reading and Storing the Documentation Data\n",
    "\n",
    "In this section, we'll read the markdown `.md` file data, collect it, and store it for processing. We can do this by reading through all of the `.md` files in a directory and reading them into plain text format, then storing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arielbarnea/Documents/Parallel Works/pw-abarnea/semantic_search/doc_reader.py:18: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 18 of the file /Users/arielbarnea/Documents/Parallel Works/pw-abarnea/semantic_search/doc_reader.py. To get rid of this warning, pass the additional argument 'features=\"html5lib\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  text = ''.join(BeautifulSoup(html).findAll(text=True))\n"
     ]
    }
   ],
   "source": [
    "import doc_reader as reader\n",
    "\n",
    "doc_data = reader.collect_doc_data(\"docs/docs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Cleaning the Documentation Data\n",
    "\n",
    "In this section, we'll take our collected and stored documentation data from Step 1 and clean it up so we can use it. This could include removing HTML tags, removing punctuation and special characters, removing extra whitespaces from the text, making all of our text lowercase for semantic searching, and catching any mispellings in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import md_cleaner as cleaner\n",
    "\n",
    "cleaned_doc_data = cleaner.clean_doc_data(doc_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Pre-processing the Documentation Data\n",
    "\n",
    "In this section, we'll take our cleaned documentation data from Step 2 and pre-process it by tokenization, stemming lemmatization, and stop-word removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmd_preprocessor\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpreprocessor\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m preproc_docs \u001b[39m=\u001b[39m preprocessor\u001b[39m.\u001b[39;49mpreprocess_doc_data(cleaned_doc_data)\n",
      "File \u001b[0;32m~/Documents/Parallel Works/pw-abarnea/semantic_search/md_preprocessor.py:72\u001b[0m, in \u001b[0;36mpreprocess_doc_data\u001b[0;34m(cleaned_doc_data)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_doc_data\u001b[39m(cleaned_doc_data: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[\u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m]]:\n\u001b[1;32m     61\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m    Preprocesses the full documentation data set by tokenizing each string entry\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m    in the inputted documentation data, then removing stop words and lemmatizing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m        preproc_doc_data (list[list[str]]) : full pre-processed documentation data\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m {file : preprocess_str(content) \u001b[39mfor\u001b[39;49;00m file, content \u001b[39min\u001b[39;49;00m cleaned_doc_data\u001b[39m.\u001b[39;49mitems()}\n",
      "File \u001b[0;32m~/Documents/Parallel Works/pw-abarnea/semantic_search/md_preprocessor.py:72\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_doc_data\u001b[39m(cleaned_doc_data: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[\u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m]]:\n\u001b[1;32m     61\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m    Preprocesses the full documentation data set by tokenizing each string entry\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m    in the inputted documentation data, then removing stop words and lemmatizing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m        preproc_doc_data (list[list[str]]) : full pre-processed documentation data\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m {file : preprocess_str(content) \u001b[39mfor\u001b[39;00m file, content \u001b[39min\u001b[39;00m cleaned_doc_data\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/Documents/Parallel Works/pw-abarnea/semantic_search/md_preprocessor.py:56\u001b[0m, in \u001b[0;36mpreprocess_str\u001b[0;34m(cleaned_str)\u001b[0m\n\u001b[1;32m     54\u001b[0m preproc_tokens \u001b[39m=\u001b[39m tokens\n\u001b[1;32m     55\u001b[0m \u001b[39mfor\u001b[39;00m func \u001b[39min\u001b[39;00m preproc_funcs:\n\u001b[0;32m---> 56\u001b[0m     preproc_tokens \u001b[39m=\u001b[39m func(preproc_tokens)\n\u001b[1;32m     58\u001b[0m \u001b[39mreturn\u001b[39;00m preproc_tokens\n",
      "File \u001b[0;32m~/Documents/Parallel Works/pw-abarnea/semantic_search/md_preprocessor.py:37\u001b[0m, in \u001b[0;36m_clean_tokens\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_clean_tokens\u001b[39m(tokens: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m     34\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m    Clean tokens again.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m [re\u001b[39m.\u001b[39;49msub(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m[^a-zA-Z0-9]\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, token) \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m tokens \u001b[39mif\u001b[39;49;00m token]\n",
      "File \u001b[0;32m~/Documents/Parallel Works/pw-abarnea/semantic_search/md_preprocessor.py:37\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_clean_tokens\u001b[39m(tokens: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m     34\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m    Clean tokens again.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m [re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[^a-zA-Z0-9]\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, token) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m token]\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "import md_preprocessor as preprocessor\n",
    "\n",
    "preproc_docs = preprocessor.preprocess_doc_data(cleaned_doc_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Implementing Semantic Search with Word2Vec\n",
    "\n",
    "Now, we can use `Gensim` to implement the semantic searching of the cleaned and pre-processed documentation data with its Word2Vec algorithm. This basically maps words and phrases to dense vector representations in a high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.downloader\n",
    "\n",
    "# pretrained_model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "corpus = list(preproc_docs.values())\n",
    "model = Word2Vec(corpus, vector_size=500, window=5, min_count=5, workers=4)\n",
    "# model.build_vocab_from_freq(pretrained_model.key_to_index, corpus_count=len(corpus), update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = {}\n",
    "\n",
    "for filename, tokens in preproc_docs.items():\n",
    "    embeddings = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if embeddings:\n",
    "        document_embeddings[filename] = gensim.matutils.unitvec(np.mean(embeddings, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query_str: str):\n",
    "    \"\"\"\n",
    "    Performs a similarity search on the inputted query against the given Word2Vec model.\n",
    "\n",
    "    Parameters:\n",
    "        query_str (str) : user query\n",
    "    \n",
    "    Returns:\n",
    "        similar_docs (list[str]) : most similar docs in order\n",
    "    \"\"\"\n",
    "    query_tokens = preprocess_str(clean_str(query_str))\n",
    "    average_vec_rep = [model.wv[token] for token in query_tokens if token in model.wv]\n",
    "    query_embedding = gensim.matutils.unitvec(np.mean(average_vec_rep, axis=0))\n",
    "\n",
    "    similar_docs = []\n",
    "    for filename, doc_embedding in document_embeddings.items():\n",
    "        similarity_score = np.dot(doc_embedding, query_embedding)\n",
    "        similar_docs.append((filename, similarity_score))\n",
    "    \n",
    "    similar_docs = sorted(similar_docs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_files(query, top_k=5, include_score=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Gets the top 'k' relevant files from an inputted query. Defaults to top\n",
    "    5 most relevant files.\n",
    "\n",
    "    Parameters:\n",
    "        query (str) : question to search PW documentation for\n",
    "        top_k (int) : top 'k' most relevant files to return (default: 5)\n",
    "        include_score (bool) : if True, includes similarity score of file\n",
    "        verbose (bool) : if True, prints files in addition to returning\n",
    "    \n",
    "    Returns:\n",
    "        rel_files (list) : top 'k' most relevant files\n",
    "    \"\"\"\n",
    "    similar_docs = run_query(query)\n",
    "\n",
    "    if include_score:\n",
    "        rel_files = similar_docs[:top_k]\n",
    "        if verbose:\n",
    "            print(f\"Top {top_k} most relevant files to your query with similarity scores included:\\n\")\n",
    "            for i, (file, sim_score) in enumerate(rel_files):\n",
    "                print(f\"{i + 1}. {file}: {sim_score}\")\n",
    "        return rel_files\n",
    "    else:\n",
    "        rel_files = [filename for filename, _ in similar_docs[:top_k]]\n",
    "        if verbose:\n",
    "            print(f\"Top {top_k} most relevant files to your query:\\n\")\n",
    "            for i, file in enumerate(rel_files):\n",
    "                print(f\"{i + 1}. {file}\")\n",
    "    return rel_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most relevant files to your query with similarity scores included:\n",
      "\n",
      "1. configuring-storage.md: 0.9999051094055176\n",
      "2. configuring-clusters.md: 0.99989914894104\n",
      "3. navigating-the-admin-panel.md: 0.9998986124992371\n",
      "4. logging-in-controller.md: 0.9998984932899475\n",
      "5. configuring-instances.md: 0.9998974800109863\n"
     ]
    }
   ],
   "source": [
    "query = \"Where do I look for a question about monitoring my costs?\"\n",
    "\n",
    "get_relevant_files(query, include_score=True, verbose=True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
