{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search TF-IDF Model First Draft\n",
    "\n",
    "This Jupyter notebook is meant to serve as an introduction to reading Github `.md` documentation and analyzing it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doc_reader as reader\n",
    "\n",
    "doc_data = reader.collect_doc_data(\"docs/docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import md_cleaner as cleaner\n",
    "import md_preprocessor as preprocessor\n",
    "\n",
    "def clean_and_preproc_data(input_data):\n",
    "    \"\"\"\n",
    "    Helper function to combine cleaning and preprocessing of data.\n",
    "\n",
    "    Parameters:\n",
    "        doc_data (dict | str) : raw documentation data or string\n",
    "    \n",
    "    Returns:\n",
    "        cp_doc_data (dict | str) : cleaned and preprocessed doc data or string\n",
    "    \"\"\"\n",
    "    if isinstance(input_data, dict):\n",
    "        cleaned_data = cleaner.clean_doc_data(input_data)\n",
    "        cp_data = preprocessor.preprocess_doc_data(cleaned_data)\n",
    "    elif isinstance(input_data, str):\n",
    "        cleaned_data = cleaner.clean_str(input_data)\n",
    "        cp_data = preprocessor.preprocess_str(cleaned_data)\n",
    "\n",
    "    return cp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_doc_data = clean_and_preproc_data(doc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def initialize_vectorizer(docs):\n",
    "    \"\"\"\n",
    "    Initializes a TF-IDF vectorizer model on inputted documents.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, stop_words='english')\n",
    "    tokenized_corpus = list(docs.values())\n",
    "    corpus = [' '.join(tokens) for tokens in tokenized_corpus]\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "vectorizer, tfidf_matrix = initialize_vectorizer(cp_doc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, docs, vectorizer, tfidf_matrix):\n",
    "    \"\"\"\n",
    "    Runs a semantic search with a query on inputted docs.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The query string\n",
    "        docs (list[str]): List of document names corresponding to the tfidf_matrix rows\n",
    "        vectorizer (TfidfVectorizer): The initialized TF-IDF vectorizer\n",
    "        tfidf_matrix (scipy.sparse.csr_matrix): The TF-IDF matrix\n",
    "\n",
    "    Returns:\n",
    "        results (list): List of tuples containing similar documents and their similarity scores\n",
    "    \"\"\"\n",
    "    corrected_query = cleaner.correct_spelling(query)\n",
    "    cp_query = clean_and_preproc_data(corrected_query)\n",
    "    query_vector = vectorizer.transform([\" \".join(cp_query)])\n",
    "    \n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix)\n",
    "    sorted_indexes = np.argsort(similarity_scores, axis=1)[0][::-1]\n",
    "\n",
    "    filenames = list(docs.keys())\n",
    "    similar_docs = [(filenames[i], similarity_scores[0][i]) for i in sorted_indexes]\n",
    "\n",
    "    return similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_files(query, docs, vectorizer, tfidf_matrix, top_k=5, include_score=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Gets the top 'k' relevant files from an inputted query. Defaults to top\n",
    "    5 most relevant files.\n",
    "\n",
    "    Parameters:\n",
    "        query (str) : question to search PW documentation for\n",
    "        top_k (int) : top 'k' most relevant files to return (default: 5)\n",
    "        include_score (bool) : if True, includes similarity score of file\n",
    "        verbose (bool) : if True, prints files in addition to returning\n",
    "    \n",
    "    Returns:\n",
    "        rel_files (list) : top 'k' most relevant files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        similar_docs = semantic_search(query, docs, vectorizer, tfidf_matrix)\n",
    "    except TypeError:\n",
    "        print(\"Your query does not match anything in our system.\")\n",
    "        return []\n",
    "\n",
    "    if include_score:\n",
    "        rel_files = similar_docs[:top_k]\n",
    "        if verbose:\n",
    "            print(f\"Top {top_k} most relevant files to your query with similarity scores included:\\n\")\n",
    "            for i, (file, sim_score) in enumerate(rel_files):\n",
    "                print(f\"{i + 1}. {file}: {sim_score}\")\n",
    "        return rel_files\n",
    "    else:\n",
    "        rel_files = [filename for filename, _ in similar_docs[:top_k]]\n",
    "        if verbose:\n",
    "            print(f\"Top {top_k} most relevant files to your query:\\n\")\n",
    "            for i, file in enumerate(rel_files):\n",
    "                print(f\"{i + 1}. {file}\")\n",
    "    return rel_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most relevant files to your query with similarity scores included:\n",
      "\n",
      "1. creating-storage.md: 0.1989001322520514\n",
      "2. adding-cloud-accounts.md: 0.19838891885986862\n",
      "3. transferring-data-aws.md: 0.19001672142716608\n",
      "4. starting-stopping-clusters.md: 0.13871795200352793\n",
      "5. configuring-billing-infrastructure.md: 0.10918386509903194\n"
     ]
    }
   ],
   "source": [
    "query = \"Where can I find AWS key buckets?\"\n",
    "\n",
    "get_relevant_files(query, cp_doc_data, vectorizer, tfidf_matrix, include_score=True, verbose=True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
