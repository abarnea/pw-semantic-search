{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search TF-IDF Model First Draft\n",
    "\n",
    "This Jupyter notebook is meant to serve as an introduction to reading Github `.md` documentation and analyzing it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doc_reader as reader\n",
    "\n",
    "doc_data = reader.collect_doc_data(\"docs/docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import md_cleaner as cleaner\n",
    "import md_preprocessor as preprocessor\n",
    "\n",
    "def clean_and_preproc_data(input_data):\n",
    "    \"\"\"\n",
    "    Helper function to combine cleaning and preprocessing of data.\n",
    "\n",
    "    Parameters:\n",
    "        doc_data (dict | str) : raw documentation data or string\n",
    "    \n",
    "    Returns:\n",
    "        cp_doc_data (dict | str) : cleaned and preprocessed doc data or string\n",
    "    \"\"\"\n",
    "    if isinstance(input_data, dict):\n",
    "        cleaned_data = cleaner.clean_doc_data(input_data)\n",
    "        cp_data = preprocessor.preprocess_doc_data(cleaned_data)\n",
    "    elif isinstance(input_data, str):\n",
    "        cleaned_data = cleaner.clean_str(input_data)\n",
    "        cp_data = preprocessor.preprocess_str(cleaned_data)\n",
    "\n",
    "    return cp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_doc_data = clean_and_preproc_data(doc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['working cloud snapshot cloud snapshot let make persistent change operating system image running cluster node using image provided parallel work base stage automation install additional software enable additional service creating cloud snapshot navigate account setting username account profile setting click cloud snapshot cloud snapshot click new snapshot snapshot configuration setting several configurable parameter cloud snapshot outlined type use dropdown menu select whether snapshot built aws azure google cloud account use dropdown menu select cloud account provision snapshot user menu left default option unless youre member multiple organization group use dropdown menu select group name organization us allocate cost menu especially important organization us multiple group youre sure group select contact u organization pw platform administrator snapshot region use dropdown menu select region snapshot provisioned run multiple region want provision snapshot wherever run cluster base image use dropdown menu select pw image base snapshot typically menu populated suggested version default root disk size gb use field enter size snapshot root disk default field set 100 typically wont change value unless run capacity issue name use field enter name snapshot listed pw platform description use field enter description provide additional detail snapshot snapshot build script enter script snapshot build script run make addition base pw image use script install additional package yum repository clone github repository download file url example build script provided bash binbash get useful environment info echo username user echo home dir home echo workdir pwd clone public github repository git clone httpsgithubcomhpciorgit usrlocalsrcior install anaconda initialize login wget httpsrepoanacondacomarchiveanaconda3202303linuxx8664sh sh anaconda3202303linuxx8664sh b p usrlocalanaconda3 symlink conda executable main usrlocalbin directory ln usrlocalanaconda3binconda usrlocalbinconda install jupyter conda install jupyter package installed anaconda root directory symlink need ln usrlocalanaconda3binjupyterlab usrlocalbinjupyterlab ln usrlocalanaconda3binjupyter usrlocalbinjupyter install r r studio server specify r version want export rversion413 download install r package curl httpscdnrstudiocomrcentos7pkgsrrversion11x8664rpm yum install rrversion11x8664rpm note package installs r optr413binr install r studio package wget httpsdownload2rstudioorgservercentos7x8664rstudioserverrhel2023030386x8664rpm yum install rstudioserverrhel2023030386x8664rpm build script print basic environment information clone public ior repository github usrlocalsrcior download run conda installer download install r r studio server link executables common path location youve provided build script click create snapshot save configuration provisioning cloud snapshot clicking create snapshot page update provisioning log two new button save snapshot config provision snapshot click save snapshot config making configuration change want save including change main parameter click provision snapshot start building snapshot image replace snapshot build script provisioning log monitor snapshot build snapshot build process work creating temporary virtual machine vm run pw base image provided script run make addition image saved separate snapshot snapshot done building provisioning log show message identifying name image csp account error build prevented completing also visible log build successful provisioning log show message json packer build completed successfully snapshotidami0a5854d925e6e558b snapshotnamepwdemohello cspaws regionuseast1 httpsparallelworksapiv2machineimages build unsuccessful provisioning log show message json build amazonebsaws errored 1 minute 57 second script exited nonzero exit status 1 allowed exit code 0 wait completed 1 minute 57 second build didnt complete successfully error amazonebsaws script exited nonzero exit status 1 allowed exit code 0 build finished artifact created packer build failed info note reprovision existing snapshot new build error previously working snapshot overwritten using cloud snapshot snapshot created csp automatically added cluster configuration setting see usercreated snapshot cluster configuration page selecting image controller setting elastic image partition setting also configure different snapshot controller partition deleting cloud snapshot delete custom snapshot navigate configuration page click deprovision snapshot dialog box appear message sure want deprovision snapshot test click deprovision snapshot dialog box provisioning log display deletion process json starting delete custom image cloud aws region useast1 project cacloudmgmt image pwdemohello ami ami023b3386461fdfad6 snapshotid snap01d59ca427982c1d8 aws ec2 deregisterimage imageid ami023b3386461fdfad6 aws ec2 deletesnapshot snapshotid snap01d59ca427982c1d8 delete completed use method still snapshot listed cloud snapshot username account cloud snapshot able reconfigure reprovision snapshot time alternatively delete snapshot list cloud snapshot click delete icon dialog box appear message sure want delete snapshot snapshot name clicking delete snapshot permanently remove snapshot account list cloud snapshot info deletion want delete snapshot list cloud snapshot first need deprovision snapshot configuration page dont snapshot still exist csp account', 'label getting support getting support youre experiencing problem pw platform help check platform status time clicking status login page bottomleft corner log submit help ticket emailing u supportparallelworkscom', 'sidebarlabel navigating admin panel title admin panel page explains feature admin panel access clicking username selecting admin info persona youll able access feature youre pw platform administrator info container much user guide say user workspace talk user account platform please note term synonymous user container use page platform setting navigate admin panel youll see platform setting default tab display essential information organization version platform also manage maintenace mode tab mode enabled admins able start stop resource enable maintenance mode click checkbox next click save platform setting change applied immediately platform remain maintenance mode deactivate user tab display user information including username email address api key active status container image account creation date use username field search user use active dropdown menu display user active inactive either option set either default use limit dropdown menu narrow listed user 10 20 50 100 option set 50 default editing user information user tab click username youd like edit next page edit several user parameter first section contains information user account container access platform identity edit user username email altering either parameter change user login credential setting control user access platform user host field identifies user host part platform account resource provisioned recommend contacting u make change parameter user container option identifies service used deploy user container use bubble select docker k8s kubernetes recommend contacting u make change parameter mfa enabled use bubble select whether user use multifactor authentication select user doesnt need multifactor authentication select myproxy work governmental agency public university research organization generally myproxy preferred type institution select duo work company organization affiliated government duo cisco service popular type institution recommend contacting u make change parameter user container image name field identifies ide service use house user container currently use theia user container recommend contacting u make change parameter user container image version field identifies image version user container running default field set latest recommend contacting u make change parameter user organization use dropdown menu assign user organization attribute control whether user admin active selecting respective checkboxes user admin enabled able access admin panel setting option affect group setting orgadmin information please see role user active enabled able log platform username password unchecking active disable user access platform action click update user container button migrate user container latest image click kill user container button restart user container click remove team button remove user assigned group info note change parameter click save leaving page change applied immediately set password reset user login credential change parameter click save leaving page change applied immediately contact info change user contact information including display name platform url geographical location information visible platform admins affect user login credential change parameter click save leaving page change applied immediately setting user image user tab select user checkboxes click set image dialog box appear enter image name image version click apply massupdate change applied immediately updating user image user tab select user checkboxes click update inplace dialog box appear confirm youve selected user want update click apply massupdate change applied immediately group tab display information group including name organization number member allocation amount percentage allocation used state creation date id number function managing group completed tab well key key tab show information cloud account key including name organization owns key creation date key id manage cloud account key tab click name key edit name description next page click save making change also permanently delete key platform clicking delete button please note youve used key provision infrastructure wont able delete key youve deprovisioned infrastructure info note need add key platform please see adding cloud account instance tab display information instance including csp organization owns key region instance id creation date instance name instance type private ip public ip cloud state estimated cloud state time pw state please note instance list wider monitor included screenshot show field listed see information instance scroll bottom page use horizontal scroll bar show column configure instance type platform organization information please see configuring instance type container tab display information user container including container name user container image image version current image version platform creation date type docker kubernetes killing user container container tab select user checkboxes click kill container dialog box appear confirm youve selected container want kill click apply massupdate change applied immediately report tab display information report including type report email address receives report report monitor frequency creator creation date creating report report tab click add new report next page edit parameter listed use type dropdown menu select type report want generate summary project monthly user application use field enter username report monitor use email field enter report recipient email address use frequency bubble select frequently report generated daily weekly monthly event time creation test time creation disable end report time', 'label platform slug platform platform give user ability create manage elastic resource pool highperformance computing hpc cluster virtually cloud service provider platform designed accessible universally useful user run scriptscalled workflowsthat defined variety programming language across multiple pool cluster pw customer use cloud account deploy cloud hpc resource via pw platform meaning hpc resource customerdeployed solution pw platform deployed different location ie cloud onpremise deployed customer directly goal make highperformance simulation modeling computation simple possible working towards goal every day working cloud resource process using cloud resource typically follows step log pw platform create start cluster configuration best suit work transfer data cloud onpremise location object storage amazon web service aws s3 bucket google cloud storage gc bucket microsoft azure blob container complete computation transfer data back onpremise location object storage shut cluster essential cloud concept cluster standard hpc cluster wellsuited supporting execution wide range parallel application including mpi openmp gpubased application well various hybrid combination cloudbased cluster run variety job scheduler providing environment thats familiar user traditional hpc cluster pw platform cluster one controller node sends job compute node grouped partition pool collection compute resource group nearly identical cloud worker instance pool directly managed pw infrastructure elastic fashion worker pool connect directly pw platform workflow script written variety language programming model typically workflow orchestrate execution application relevant domain whether thats computational fluid dynamic mechanical engineer molecular modeling biochemist workflow run either pool cluster depending need workflow several demonstration template workflow available pw marketplace see navigating platform marketplace information system email pw platform sends notification user via email system configured send daily report regarding cloud resource usage additionally group member receive email usage exceeds group budget allocation threshold 85 90 95 995 email service provider may send pw notification junk spam folder advise periodically check folder system message email service provider add system email address noreplyparallelworkscom safe sender list address book redirect pw email inbox gmail user also use label service redirect pw platform email instruction process found guide constantly update platform adding feature make easier complete largescale project documentation work progress following instruction information updated regularly work improve platform user experience would love hear feedback feel free share request additional feature thought platform feedbackparallelworkscom', 'label navigating platform navigating platform section introduces major feature platform help new user get oriented logging navigate cloudparallelworks youll see login page url use may different cloudparallelworks depending organization whether use onpremise dedicated instance platform log password box appear enter username email address click next enter username see error message username email found please reach u supportparallelworkscom assistance home page log pw platform youll see compute tab default youll regularly interact following platform feature compute tab monitor resource workflow resource tab create manage resource workflow tab add manage workflow user workspace panel labeled explorer pw access terminal interface file job youve run platform user menu give access account setting pw marketplace compute compute tab serf primary activity monitor pw platform select favorited workflow adjust resource requirement view past run result page four main box tab favorite workflow store favorited workflow easy access workflow monitor keep track current past workflow run cancel run run workflow resource monitor see many node active long theyve running computing resource stop start resource configure clicking gear icon box expanded default collapse box clicking title arrow next title also collapse individual item computing resource clicking title resource resource tab access current computing resource add new one also manage existing resource favoriting duplicating editing deleting sharing user organization information add configure new resource see creating cluster configuring cluster workflow workflow tab display preconfigured workflow tool template run platform workflow build add marketplace appear automatically also manage existing workflow favoriting duplicating editing deleting sharing user organization user workspace ide explorer pw panel right side screen user workspace also known integrated development environment ide click ide icon panel expand fit screen showing terminal pane bottom page default ide powerful tool allowing view job output error file run code debug workflow ide based theia use terminal ide run command much like terminal macos command prompt window hide ide completely clicking arrow icon appears bottom right corner screen ide collapsed marketplace marketplace page select preconfigured workflow project navigate clicking username selecting marketplace dropdown menu sort preconfigured workflow using either search bar category pane default item category collapsed expand click account account page manage login information cloud snapshot cloud account navigate clicking username selecting account dropdown menu section account page detailed profile change password well display name typing name field username email locked assigned account created api key access api key youll use api key time need interact api automation task cloud snapshot create access cloud snapshot create cloud snapshot temporary virtual machine vm executes script system take snapshot vm us create image access image dropdown menu possible select machine image example organization us application take hour install create cloud snapshot computer installation use snapshot avoid installation process every time user need application information see working cloud snapshot cloud account connect pw account existing cloud account aws google azure use private ssh key existing resource feature useful need connect preconfigured resource either cloud account another user organization information see linking cloud account coming soon', 'restarting workspace user workspace dedicated environment starting cluster running workflow accessing data platform user workspace separate work platform never affect another user work page explains restart user workspace useful youre experiencing issue like cluster wont start workflow wont run info note running cluster job disrupted restart workspace navigate account setting username account profile setting click restart workspace dialog box appear message sure want restart workspace click restart workspace restarted youll see message user workspace killed info troubleshooting restarted workspace experiencing error arent resolved restarting please contact u pw platform administrator', 'sidebarlabel attaching storage title attaching storage youve created configured storage option attach one cluster navigate cluster configuration page click definition tab attached storage section bottom page click add attached storage two new parameter appear storage mount point use storage dropdown menu select storage option attach cluster please note storage option match cluster csp shown example aws cluster show aws fsx lustre aws elastic file system use mount point field enter directory storage mounted cluster controller compute node please note mount point absolute path info note add remove persistent storage option cluster running ephemeral storage verifying storage mount youve logged cluster run following command verify storage mounted correctly bash df h enter command see storage mounted mount point defined mount updated every 45 second recently attached storage option cluster may short delay see storage mount list', 'deployment base infrastructure order provision cloud cluster must first deploy base set infrastructure csp account throughout documentation refer base infrastructure base infrastructure shared multiple cluster varies csp essence think networking stack cluster deployed top find specific detail implementation base infrastructure csp aws infrastructure deploying base infrastructure provision following aws virtual private cloud vpc internet gateway public private subnets public private route table route network address translation nat gateway elastic ip address nat gateway route 53 hosted zone security group schematic show pwdeployed cluster aws cloud controller compute node shown availability zone useast1a inside useast1 region compute node region use single nat gateway outbound internet connectivity nat gateway preprovisioned part base infrastructure ami used controller compute node inside parallel work aws account info expense information creating nat gateway result additional monthly charge aws account information please see aws pricing list', 'sidebarlabel creating storage title import bsfilleyefill reacticonsbs creating storage navigate storage tab click add storage next page five storage option choose aws fsx lustre lustre google lustre azure aws s3 bucket google cloud bucket select filesystem enter storage name short description tag thumbnail optional descriptor dont upload thumbnail image storage icon default icon option youve chosen storage type select whether storage ephemeral persistent please note cloud object store aws s3 bucket google cloud bucket ephemeral click add storage next youll see default setting page new storage please see configuring storage next step info note create persistent storage option appear storage resource box compute tab hide storage option compute tab clicking eye icon storage tab however ephemeral storage option appear compute tab started independently must attached cluster storage created started deleted along cluster information please see storage type type storage use lustre high inputoutput file system recommend using lustre storage option youre completing compute work need type performance storage option equivalent contrib work well general use', 'sidebarlabel configuring storage title accessing storage configuration setting youll see storage configuration page immediately creating storage option also access configuration setting storage tab click name existing storage configure storage configuration page navigate storage configuration page five tab customization session default youll see session tab navigate storage setting tab show previous session using storage well provision deletion log info storage provisioning log persistent storage option see creation deletion log tab log current previous session displayed persistent storage option active youll see cluster storage attached ephemeral storage option session page instead see creation log navigating resource resourcename session log storage information persistent ephemeral storage please see storage type definition adjust configuration parameter storage information see general setting  json tab show code version storage configuration setting manually adjust parameter seen definition tab property tab show name entered storage created change name upload new thumbnail cluster sharing tab let share storage option user organization assigned group general setting storage option setting definition tab configuration page cloud infrastructure use dropdown menu choose infrastructure determines region storage created group use dropdown menu select group name organization us allocate cost menu especially important organization us multiple group youre sure group select contact u organization pw platform administrator lustre option aws fsx lustre configuration parameter exclusive aws fsx lustre availability zone use dropdown menu select availability zone infrastructure deployed availability zone refers isolated location inside region recommend storage availability zone match availability zone cluster youre pairing storage storage capacity gb use field enter total capacity storage field set 1200 default 1200 minimum amount storage must increased increment 2400 please note altering storage size affect estimated hourly cost file system deployment use dropdown menu select type file system storage deployed currently option scratch persistent scratch file system meant shorterterm workload provide higher throughput per tib storage capacity scratch file system fails data replicated persistent file system meant longterm workload file system fails data automatically replicate availability zone file system compression use dropdown menu select whether file system compressed currently option lz4 none lz4 compress data writing lustre reading lustre lz4 decompresses data minimally impacting performance using lz4 improve read write performance well reduce storage capacity s3 import path optional use field enter s3 bucket path import data format field s3bucketname s3 export path optional use field enter s3 bucket path export data format field s3bucketname lustre option lustre google configuration parameter exclusive lustre google region use dropdown menu select region storage deployed region represents geographic area zone use dropdown menu select zone use storage zone refers isolated location inside region recommend storage zone match zone cluster youre pairing storage md node use field enter number node storage md field set 1 default sufficient workload info md md stand metadata service lustre file system md manages namespace hierarchy record layout file handle object allocation object storage target osts md field largely affect data transfer speed md machine type use dropdown menu select machine type md node machine type determines cpu amount memory available certain machine type may also specialty hardware gpus lowlatency networking option please note altering md machine type may affect estimated hourly cost md boot disk type use dropdown menu select type disk md node use currently option localssd pdbalanced pdstandard pdssd generally speaking ssd option faster balanced option balanced option faster standard option please note altering boot disk type slightly affect estimated hourly cost md boot disk size gb use field enter total capacity md boot disk storage field set 20 default please note altering boot disk size slightly affect estimated hourly cost mdt disk type use dropdown menu select type disk mdt node use currently option localssd pdbalanced pdstandard pdssd generally speaking ssd option faster balanced option balanced option faster standard option please note altering boot disk type slightly affect estimated hourly cost info mdt mdt stand metadata target metadata content stored mdts mdt field largely affect data transfer speed mdt disk size gb optional use field enter total capacity mdt boot disk storage please note altering boot disk size slightly affect estimated hourly cost os node use field enter number node storage os field set 2 default changing number os disk node change storage capacity overall throughput youre interested tuning lustre file system parameter well os disk type strongly affect storage performance info os os stand object storage server lustre file system os provide bulk data storage file content os machine type use dropdown menu select machine type os node machine type determines cpu amount memory available certain machine type may also specialty hardware gpus lowlatency networking option please note altering os machine type may affect estimated hourly cost os boot disk type use dropdown menu select type disk os node use currently option localssd pdbalanced pdstandard pdssd generally speaking ssd option faster balanced option balanced option faster standard option please note altering boot disk type slightly affect estimated hourly cost os boot disk size gb use field enter total capacity os boot disk storage field set 20 default please note altering boot disk size slightly affect estimated hourly cost ost disk type use dropdown menu select type disk ost node currently option localssd pdbalanced pdstandard pdssd generally speaking ssd option faster balanced option balanced option faster standard option please note altering ost disk type affect estimated hourly cost info ost ost stand object storage target ost set storage volume os provides access ost disk size gb use field enter total capacity ost boot disk storage field set 1500 default please note altering boot disk size slightly affect estimated hourly cost lustre version use field enter version lustre software storage use field set latestrelease default recommend using setting best performance lustre image use dropdown menu select lustre image storage use image prepackaged software engineer installed maximize performance recommend selecting latest best performance gvnic use toggle button enable google virtual network interface card gvnic support higher network bandwidth 50100 gbps parameter affect md os node storage feature toggled independently gvnic option cluster cluster configuration affected information see google documentation gvnic tier 1 use toggle button enable tier1 increase maximum egress bandwidth upload speed 50100 gps tier1 egress bandwidth range 1032 gbps parameter affect md os node please note tier1 supported gvnic also active try start tier1 pw platform display error message tier1 supported gvnic information see google documentation tier1 lustre option lustre azure configuration parameter exclusive lustre azure region field identifies region storage deployed please note field locked region determined cloud infrastructure selected lustre image use dropdown menu select lustre image storage use image prepackaged software engineer installed maximize performance recommend selecting latest best performance md node use field enter number node storage md field set 1 default sufficient workload info md md stand metadata service lustre file system md manages namespace hierarchy record layout file handle object allocation object storage target osts md field largely affect data transfer speed md machine type use dropdown menu select machine type md node machine type determines cpu amount memory available certain machine type may also specialty hardware gpus lowlatency networking option please note altering md machine type may affect estimated hourly cost md boot disk type use dropdown menu select type disk md node use currently option localssd pdbalanced pdstandard pdssd generally speaking ssd option faster balanced option balanced option faster standard option please note altering boot disk type slightly affect estimated hourly cost md boot disk size use field enter total capacity md boot disk storage field set 20 default please note altering boot disk size slightly affect estimated hourly cost os node use field enter number node storage os field set 2 default changing number os disk node change storage capacity overall throughput youre interested tuning lustre file system parameter well os disk type strongly affect storage performance info os os stand object storage server lustre file system os provide bulk data storage file content os machine type use dropdown menu select machine type os node machine type determines cpu amount memory available certain machine type may also specialty hardware gpus lowlatency networking option please note altering os machine type may affect estimated hourly cost os boot disk type use dropdown menu select type disk os node use currently option localssd pdbalanced pdstandard pdssd generally speaking ssd option faster balanced option balanced option faster standard option please note altering boot disk type slightly affect estimated hourly cost os boot disk size use field enter total capacity os boot disk storage field set 20 default please note altering boot disk size slightly affect estimated hourly cost accelerated networking use toggle button enable accelerated networking improves networking performance large workload multiple cloud cluster information see azure documentation accelerated networking cloud object store aws s3 bucket configuration parameter aws s3 bucket explained region use dropdown menu select region storage deployed region represents geographic area cloud object store google cloud bucket configuration parameter google cloud bucket explained region use dropdown menu select region storage deployed region represents geographic area', 'transferring data tofrom aws s3 storage working aws cli transfer data tofrom s3 bucket storage youll use aws command line interface cli aws cli preinstalled aws cluster pw platform enter command directly ide logging controller active aws cluster youre transferring data s3 bucket folder onpremise cluster remote device youll likely need install aws cli first check aws cli open terminal command line navigate data destination enter aws aws cli installed youll see message show location usrlocalbinaws aws cli installed youll see message usrbinwhich aws aws found install aws cli need install aws cli recommend following aws installation guide includes osspecific instruction linux macos window well troubleshooting tip transferring data export aws credential terminal enter export awsaccesskeyid aws access key id blank space enter export awssecretaccesskey aws access key id blank space info note please sure include quotation mark end key id character inside aws key id without quotation mark system try read command validate aws credential terminal enter aws sts getcalleridentity check system correctly registered aws credential login successful youll see aws userid account arn bash userid abcd1e2fg3h4ijklmnopq account 012345678901 arn arnawsiam 012345678901useradmin login successful enter aws access key export command also configure login credential manually aws configure information process see aws documentation environment variable configuration file list file s3 terminal enter aws s3 l s3bucketname display file bucket guide used small text file named testtxt command returned message bash demopwuserdemopwclouddatatransfer aws s3 l s3clouddatatest 20230208 163756 28 testtxt transfer file s3 terminal enter aws s3 cp s3bucketnamefilename download file s3 storage current directory youll see message bash demopwuserdemopwclouddatatransfer aws s3 cp s3clouddatatesttesttxt download s3clouddatatesttesttxt testtxt download file s3 storage specific directory enter name aws s3 cp command used storage directory example bash demopwuserdemopwclouddatatransfer aws s3 cp s3clouddatatesttesttxt storage download s3clouddatatesttesttxt storage transfer file s3 terminal enter aws s3 cp filename s3bucketname transfer file s3 bucket youll see message bash demopwuserdemopwclouddatatransfer aws s3 cp testuploadtxt s3clouddatatest upload testuploadtxt s3clouddatatesttestuploadtxt check s3 file aws s3 l s3bucketname bash demopwuserdemopwclouddatatransfer aws s3 l s3clouddatatest 20230208 163756 28 testtxt 20230208 171924 28 testuploadtxt delete file s3 terminal enter aws s3 rm s3bucketnamefilename delete file youll see message bash demopwuserdemopwclouddatatransfer aws s3 rm s3clouddatatesttestuploadtxt delete s3clouddatatesttestuploadtxt check s3 file aws s3 l s3bucketname bash demopwuserdemopwclouddatatransfer aws s3 l s3clouddatatest 20230208 163756 28 testtxt', 'label creating cluster creating cluster work pw platform need completed resource dont preconfigured resource fit need project hand create new one navigate resource tab select add resource select type resource need available elastic cluster choose type resource create cluster specific cloud service provider csp example want create elastic cluster resource aws google azure creating cluster must enter pool name display name short description tag thumbnail optional descriptor leave display name blank cluster default pool name text dont upload thumbnail image cluster icon default icon csp youve chosen youve entered information field select add resource resource creation confirmed next page message resource created configure resource page information configuring cluster generally need configure cluster specific csps see configuring cluster new cluster also appear computing resource box compute tab type resource use youre creating new resource elastic cluster typically best choice user familiar slurm comfortable using elastic cluster instead elastic pool additionally newer pw workflow run exclusively elastic cluster reason consider elastic pool legacy feature discourage user choosing create new resource csp choose depends organization workflow need whether already cloud account specific csp otherwise elastic cluster good choice start configure resource youre simply testing resource organization provided specific configuration setting project recommend using default configuration setting information see configuring cluster default pool configuration', 'label starting stopping cluster starting stopping cluster starting cluster navigate computing resource box compute tab click power button cluster start power button flash green requested status bubble turn yellow may take 15 minute aws cluster start roughly 5 minute google cluster start cluster online power button active bubble turn green cluster ready run workflow stopping cluster job finished youre ready turn running cluster click power button dialog box appear message sure want turn cluster name click turn stop cluster stop cluster data lustre local file system lost data object storage aws s3 bucket gc bucket azure blob container contrib remain important copy data youd like keep object storage remote location info note essential turn cluster youve finished work cluster run unmonitored continue accrue additional charge subtracted organization allotment', 'label configuring cluster configuring cluster user pw platform work exclusively elastic cluster cluster made controller node compute node controller delegating task compute node cluster several adjustable parameter controller compute node compute instance type node count additionally compute node grouped together partition setting information see partition setting pw platform also support optional parallel file system lustre information setting lustre account see storage coming soon accessing configuration setting access resource configuration setting compute tab navigate computing resource box click gear icon resource want configure alternatively navigate resource tab click name resource want configure resource configuration page navigate cluster configuration setting four tab customization session default youll see session tab navigate configuration setting tab show previous cluster session well provisioning deletion log info storage log session box tab youll also able see session attached ephemeral storage option multiple ephemeral storage option attached cluster youll see dropdown select ephemeral storage log youd like see deletion log ephemeral storage option combined cluster deletion log information please see storage type definition adjust cluster parameter information see general setting  json tab show code version resource configuration setting manually adjust parameter seen definition tab property tab show resource name description display name tag entered resource created adjust setting upload new thumbnail cluster sharing tab let share resource user organization assigned group please note group name specific organization information see group general setting cluster typically setting definition tab configuration page youre using existing onpremise cluster see existing cluster setting resource account use dropdown menu select account organization us specific cloud service provider default menu show resource account organization selected type cluster example screenshot show google cluster resource account automatically populated pworks gcp account group use dropdown menu select group name organization us allocate cost menu especially important organization running multiple group simultaneously youre sure group select contact u organization pw platform administrator multi user use toggle button automatically create home directory user selected group option different sharing tab allows sharing resource definition user organization access public key use box add ssh key access cluster remote device like local laptop please note key must openssh format enter public key private key information use public key see logging controller script setting optionally set script execute start cluster user bootstrap use box set script executes controller node started example set file automatically move specific folder health check use box set script run health check controller node script done running youll see error code red exit code 0 green error information see health check coming soon controller setting setting define configuration controller node region instance type o image setting differ depending type resource youre using information see cspspecific setting region use dropdown menu select region cluster deploy computing resource region represents geographic area zone use dropdown menu select zone use controller zone refers isolated location inside region info note azure cluster zone menu instance type use dropdown menu select instance type controller instance type determines cpu amount memory available machine certain instance type may also specialty hardware gpus lowlatency networking option image use dropdown menu select operating system o image cluster controller node recommend using latest version ensure uptodate software cluster latest image version includes o update software required connect pw platform image disk name fixed organization us snapshot disk field identify snapshot example organization may specific application user need complete work administrator may create snapshot disk make apps available user whenever cluster start name snapshot would image disk name please note make change directory cluster change lost turn cluster change affect snapshot user work image disk use field enter number image disk youll need cluster typically youll either enter 1 need directory image disk name 0 would like disable image disk image disk size gb use field enter amount storage image disk size depends size snapshot provided organization administrator partition setting create partition cluster send work differently configured set worker node partition especially useful youre working project need fewer node specific task example running simulation model small dataset required twice amount gpu power render properly info note must least one partition cluster click add partition list new setting expand typically partition following configuration option setting differ depending type resource youre using information see cspspecific setting name use box name partition sure use unique name partition create partition never named default instance type use dropdown menu select configuration partition option work way controller instance type max node use field enter max number node partition default use toggle button specify whether partition default location running job information running job specific partition see submitting job info default partition feature important create multiple partition create one partition automatically set default changed shown screenshot spot use toggle button specify whether partition spot instance spot instance cost effective make use resource already available currently unused however spot instance disrupted another user take available resource time reason recommend using spot instance risk elastic image use dropdown menu select operating system image partition recommend using latest version zone use dropdown menu select zone within selected region configure partition run different zone controller node selecting different zone multiple partition increase chance provisioned resource available cloud provider performance penalty compute node need communicate across zone slurm setting pw platform us slurm manage job controller compute node setting determine slurm behaves cluster node please note numerical value enter field measured second suspend time use field set long slurm wait shutting idle node field set 300 default resume timeout use field set maximum amount time slurm try start node node dont start end set time slurm end initialization attempt field set 1200 default suspend timeout use field set long slurm wait make node available shutting field set 300 default return service use dropdown menu select node returned service non responsive option mean node become available set nonresponsive reason option mean node become available set reason including low memory unexpected reboot nonresponsive field set non responsive default cspspecific setting cloud service provider csp build configures resource differently cluster pw platform setting correspond csps model cloud service cspspecific parameter outlined please note cspspecific setting also appear option inside partition setting cluster aws efa use toggle button enable elastic fabric adapter efa improves interinstance network performance efa useful need scale hpc machinelearning application thousand cpu gpus please note efa supported instance type information list supported instance type see aws documentation efa azure export filesystem use field enter name network file system nfs existing system external device thats available read andor write access cluster want set nfs please contact u pw platform administrator nfs size use field enter size nfs please note value nfs size image disk size must accelerated networking use toggle button enable accelerated networking improves networking performance large workload multiple cloud cluster information see azure documentation accelerated networking google gvnic use toggle button enable google virtual network interface card gvnic support higher network bandwidth 50100 gbps please note gvnic supported instance type information list supported instance type see google documentation gvnic tier1 use toggle button enable tier1 increase maximum egress bandwidth upload speed 50100 gps depending size instance tier1 egress bandwidth range 1032 gbps please note tier1 supported gvnic also active try start tier1 pw platform display error message tier1 supported gvnic information see google documentation tier1 migrate maintenance toggle button enables live migration whenever virtual machine host undergoes maintenance meaning google migrate virtual machine another host without downtime please note gpu spot instance live migrated supported recommend turning feature information see google documentation live migration existing cluster setting typically create existing cluster youll connecting onpremise cluster associated organization setting specific type cluster outlined youre unsure choose option contact organization pw platform administrator general setting existing cluster resource account use dropdown menu select pw platform connect existing cluster default option mean platform try ssh cluster using pw account ssh key stored sshpwidrsa information pw ssh key see documentation pin password option creates dialog box start cluster enter password user account account define username option mean platform connect cluster using credential multi factor use toggle button youre connecting cluster mfa enabled turn resource mfa enabled dialog box appear prompting enter mfa code button different option resource account toggle mfa platform connect existing cluster using ssh key pw account mfa credential jump node use toggle button youre connecting cluster jump node enabled jump nodealso called host node bastion node login nodeis highsecurity server allows user access private machine network enable feature two new field appear jump node user jump node host organization credential need group use dropdown menu select group name organization us allocate cost menu especially important organization us multiple group youre sure group select contact u organization pw platform administrator cluster configuration setting existing cluster username use field enter username assigned cluster info username substitution existing cluster enter user box pw platform automatically substitute username field example username jdoe pw platform automatically substitute user jdoe working directory field cluster login node use field enter ip address host name cluster working directory use field enter directory youll accessing completing work cluster default field set homeslurmusername max worker use field enter maximum number compute node need complete work internal ipnetwork name optional use field specify internal ip address network name compute node use communicate controller node use feature organization configured cluster compute node send information ip address controller default ip address run command ifconfig cluster logging controller see available ip address scheduler type use dropdown menu select type job scheduler cluster us currently existing cluster resource type support slurm pb default configuration youre simply testing resource organization provided specific configuration setting group recommend using default configuration setting allow resource run project optimal performance set reset resource default pool configuration navigate resource tab click name resource want edit configuration page open click restore configuration button click restore configuration dialog box appear message restore configuration default value dropdown menu dialog box click configuration labeled benchmark click restore click save resource info existing cluster existing cluster restore configuration button info extra lustre expense use default configuration setting cluster automatically equipped lustre file system lustre powerful generally increase cost significantly google azure cluster please feel free turn lustre dont need particularly youre transferring small file simply testing resource', 'submitting job via slurm info job two way submit job cluster using workflow terminal commandline interface workflow option please see running workflow youve started cluster log controller preferred method quickest way submit job transfer file cluster run command sbatch example submitted file demotest1sbatch sbatch shell demodemocluster60 l demotest1sbatch demodemocluster60 sbatch demotest1sbatch submitted batch job 2 submitting job watch progress command watch squeue update every two second job status st column shell every 20 squeue jobid partition name user st time node nodelistreason 4 testpart test demo cf 008 2 demodemocluster00060100010002 also use watch sinfoechosqueue want see general cluster information addition job progress shell every 20 sinfo echo squeue partition avail timelimit node state nodelist testpartition1 infinite 2 mix demodemocluster00060100010002 testpartition1 infinite 3 idle demodemocluster00060100030005 testpartition2 infinite 5 idle demodemocluster00060200010005 jobid partition name user st time node nodelistreason 4 testpart test demo cf 026 2 demodemocluster00060100010002 using watch squeue watch sinfoechosqueue st column show cf node configure row beneath jobid clear job finished shell every 20 sinfo echo squeue partition avail timelimit node state nodelist testpartition1 infinite 2 idle demodemocluster00060100010002 testpartition1 infinite 3 idle demodemocluster00060100030005 testpartition2 infinite 5 idle demodemocluster00060200010005 jobid partition name user st time node nodelistreason job finished check output cat filename file demotest1sbatch inlcuded instruction send completed job data stdout file error stderr file shell demodemocluster60 l demotest1sbatch stderr stdout demodemocluster60 cat stderr demodemocluster60 cat stdout demodemocluster0006010001 demodemocluster0006010001 demodemocluster0006010002 demodemocluster0006010002 using cat stderr didnt return anything job executed without error common slurm command section give quick overview command youll use often interacting cluster use command terminal logging controller node pw platform us slurm manage job use system command extensive list option see slurms command guide also enter man front command man sacct see description list available command slurms virtual manual info job id say job id section mean job id slurm assigns work appear running many command id number worflow monitor job folder pw platform act separate identifier help u track many job weve ever run platform using command section generate new slurm job id info fault tolerance fault tolerance defined well infrastructure remains functional online even service disruption outage natural disaster pw platform cluster deletion queuebased fault tolerance cluster startup process retries fault tolerance log visible user see problem occur compute node startup request fault tolerance implemented retries via slurm default new startup attempt approximately every 20 minute job management salloc salloc retrieves resource job without executing task using command retrieves resource need signaling system reserve specified number node example salloc n 2 reserve two compute node total three node including controller salloc useful youre sharing cluster user organization using command mean job finished allocated node remain reserve use disconnect cluster meaning wait time shorter another user take control allocated node wont wait node become available wait start theyre available sbatch sbatch submits job script execute later also configure node sbatch adding option ntaskspernode specify number cpu specify maximum amount time want resource run format 000 hour minute second example sbatch demotest1sbatch ntaskspernode 5 300 would run file demotest1sbatch request 5 cpu 3 hour maximum run time srun srun executes job script use option salloc sbatch srun n specify number node ntaskspernode specify number cpu specify maximum amount time want resource run example srun n 1 pty bash would request 1 compute node open pseudoterminal creating interactive commandline session scancel scancel paired job id end pending running job job step example shell demodemocluster60 sbatch demotest1sbatch submitted batch job 6 demodemocluster60 scancel scancel error job identification provided demodemocluster60 scancel 6 cancel job disappear queue cluster management sinfo sinfo show information node partition youre using default sinfo display partition name availability time limit number node state node id number displayed usernamedemocluster00019100010005 please note enter sinfo without setting partition youll receive error message slurmloadpartitions unable contact slurm controller connect failure squeue squeue show list running pending job default squeue show job id number partition username job status number node node name queued running job also use command adjust squeues output user see one user job useryourpwusername long show nonabbreviated information add field timelimit start estimate job start time troubleshooting sacct sacct show summary user well completed running job using command display table job id number name partition status exit code whose account running many cpu using troubleshooting purpose state exitcode field running sacct especially useful determining whether node failed reach u help one support engineer may ask information see running sacct scontrol scontrol delegate command specific job id node please note many scontrol command executed user root use command job id adjust scontrols output suspend pause job process resume continue job process hold make job lower priority putting hold higher priority job run first release remove job hold list show job get detailed information job', 'logging controller cluster use one node called controller delegate task compute node carry command complete job complete many task pw platform logging controller submitting job transferring data multiple way log controller within platform logging resource name youve started cluster copy name computing resource box expand ide clicking icon terminal doesnt appear ide automatically open one selecting terminal new terminal default terminal show usernamepwuserusernamepw enter command ssh resourcenameclusterspw terminal display message warning permanently added resourcenameclusterspw ecdsa list known host well last login location logging ip address youve started cluster click ip address computing resource box copy expand ide clicking icon terminal doesnt appear ide automatically open one selecting terminal new terminal default terminal show usernamepwuserusernamepw enter command ssh usernameipaddress terminal display last login location info note time start cluster controller assigned random ip address cloud provider available address stop cluster ip address released automatically likely start cluster although unlikely youll encounter cluster ip address may occur start stop cluster frequently may receive following message warning remote host identification changed resolve issue enter command sshkeygen r controlleripaddress see following message shell demopwuserdemo sshkeygen r 35224100236 host 35224100236 found line 137 usersdemosshknownhosts updated original content retained usersdemosshknownhostsold demopwuserdemo ssh canaryparallelworks authenticity host 35224100236 cant established ed25519 key fingerprint sha256kzr9scw5qkmiceh2e5z7zhpgadououubnpkqj8uqsog key known name sure want continue connecting yesnofingerprint enter yes youll see message ip address added list known host shell warning permanently added 35224100236 ed25519 list known host last login wed jan 11 164036 2023 1046019063lightspeedhstntxsbcglobalnet demopwuserdemo outside platform logging configuration setting want access cluster terminal command line computer use ssh key log controller see dont ssh key need help cluster navigate cluster configuration setting clicking gear icon computing resource box clicking name resource tab paste public ssh key access public key dialog box select save resource start cluster cluster active navigate terminal command line device enter cluster ip address ssh usernameipaddress youll receive message sure want continue connecting yesnofingerprint enter yes terminal command line display warning permanently added resourcenameclusterspw ecdsa list known host well last login location send command controller remote device logging account setting also add ssh key pw account allow log active cluster device ssh key navigate account page username account authentication tab click add ssh key key field paste public ssh key enter name key title field click add key log active cluster navigate terminal command line device enter ssh pathtosshprivatekey usernameipaddress shell parallelworksparallelsmacbookair sshtest ssh idrsa demo3413524147 last failed login thu mar 2 094820 utc 2023 6094197104bcgoogleusercontentcom sshnotty demodemocluster70 faq dont use ssh key within platform user workspace ssh key preprovisioned logging controller ide doesnt require ssh key management public key automatically propagated controller private key stay inside workspace whenever start new cluster new key generated propagated facilitate ssh compute node process work home directory shared across node cluster adding public workspace key authorizedkeys file controller automatically allows ssh compute node access public workspace key time command cat sshpwidrsapub ide terminal see  failed login attempt first time log controller failed login attempt wont affect cluster performance theyre result platform communicates cloud service provider make resource available provisioning process platform repeatedly try ssh controller controller come online platform attempt establish tunnel connection user workspace controller user workspace key isnt available system partway bootstrap process system register connection attempt failure diagram process dont ssh key step guide finding creating ssh key device macos window info note instruction section work window use shell emulator like git bash powershell use putty manage ssh key window see use putty check ssh key open terminal macos command prompt window enter l ssh check ssh key device youve never generated ssh key youll see message l ssh file directory ssh key device theyll listed want use one existing key see get public key create ssh key enter sshkeygen youll see following message shell generating publicprivate rsa key pair enter file save key usersyourname sshidrsa type name new location press returnenter want keep default location first time youve generated ssh key use default location youll see created directory usersyourname ssh youll prompted enter passphrase ssh key following message shell enter passphrase empty passphrase enter passphrase enter passphrase press returnenter line dont want use one youll see following message along key fingerprint key randomart image shell identification saved usersyourname sshidrsa public key saved usersyourname sshidrsapub enter l ssh youll see private public ssh key get public key enter cat sshidrsapub youll see full content public ssh key copy text beginning sshrsa end local youll paste access public key box cluster configuration setting key box account authentication key caution important safety sake never store private ssh key server someone gained access private key could use access device key protects use putty public ssh key must openssh format use pw platform use putty ssh key likely saved ppk format like shell puttyuserkeyfile2 sshrsa encryption none comment rsakey20211005 publiclines 6 aaaab3nzac1yc2eaaaabjqaaaqeao7fygiresvecemn3clxkgqhg5kcqtel4vu x81zolf1p8rjsjcnlrrd0o2zfquhanfbykadso6vpg18eyhiqahgeogzuaf7 tq4oazl3yvyjkjzxqdxhnrhnjmcj438pjd69crqh4apgtupqujookje1pcpcp7fy p2y2hb0wm23k60twsml9wf2p6gsyvyxvwnlohja9luy2dtk39kcs5tmiofhi toe3zjxzytv0xllnfgjxm1gv38yia9r9fzdmxqm2hihfbt5ybb6mabbrdhvto dlha0y8oitqosmoga13mocfyllbgou65ehtnj9talkex3lgdw copying ppk key must converted openssh format easiest way puttygen typically included putty installation cli example using puttygen convert ppk key openssh format shell puttygen puttykeyppk l puttykeypub command explanation puttykeyppk source key file l openssh public key output type puttykeypub output file name result single line public key file openssh format shell cat puttykeypub sshrsa aaaab3nzac1yc2eaaaabjqaaaqeao7fygiresvecemn3clxkgqhg5kcqtel4vux81zolf1p8rjsjcnlrrd0o2zfquhanfbykadso6vpg18eyhiqahgeogzuaf7tq4oazl3yvyjkjzxqdxhnrhnjmcj438pjd69crqh4apgtupqujookje1pcpcp7fyp2y2hb0wm23k60twsml9wf2p6gsyvyxvwnlohja9luy2dtk39kcs5tmiofhitoe3zjxzytv0xllnfgjxm1gv38yia9r9fzdmxqm2hihfbt5ybb6mabbrdhvtodlha0y8oitqosmoga13mocfyllbgou65ehtnj9talkex3lgdw rsakey20211005 copy paste key cluster configuration setting pw platform', 'running workflow info job two way submit job cluster using workflow terminal commandline interface commandline interface option please see submitting job via slurm youve started cluster navigate workflow tab select workflow youd like use havent added workflow select one marketplace select username marketplace click name workflow setting box open run workflow tab enter necessary parameter click cloud icon new window resource configuration open enter necessary variable use default resource dropdown select cluster thats already running click x upperright corner save setting click execute youll see green box message 1 job successfully added queue navigate compute tab check job progress workflow monitor box display job descending order starting recently submitted job completed go job folder ide pane check job output stdout file info note example workflow used testlaunch two parameter meant testing purpose many workflow pw platform additional parameter especially execute multiple complex task need help setting parameter reach admin u time', 'sidebarlabel creating organization bootstrap script title organization bootstrap script page explains add bootstrap script organization script run whenever user organization start cluster account feature useful automating task sending platform data specific location info persona step included page completed pw platform administrator organization navigating organization bootstrap setting navigate organization setting username organization organization setting click provider provider scroll bootstrap section enter bootstrap script text box text run bash script cluster provisioning youre done click save change applied immediately info note user add bootstrap script cluster configuration setting script set instruction run user added script testing sample bootstrap script test feature simple script echo text designated file enter following command bootstrap script box bash echo hello world tmporgbootout exit 0 click save navigate compute tab start one cluster cluster active log controller ssh detailed instruction process please see logging controller logging navigate tmp read orgbootout see match echo command info note encounter error test please contact u', 'creating user page explains add new user pw platform info persona step included page completed pw platform administrator user either orgadmin orgusers role information please see group role creating new user individually method useful need add one two new user organization navigate organization setting username organization next page click user user click add user next page enter credential new user name username email password location optional youre finished click create account new user listed user tab mass importing new user method useful need add several new user navigate organization setting username organization organization setting click user user click import user next page click download template browser download csv file open spreadsheet editor like excel number template enter credential new user username password optional name email address uid optional group optional active status platform click upload template select edited file check preview new user correct click import user import log appear bottom screen error occurred importing process displayed new user listed user tab info troubleshooting macos sometimes system save csv file number file likewise excel prompt save csv file excel format instead xl however please note user import feature accepts csv file', 'configuring provider provider provider affect resource user create start limiting provider useful user organization need certain type resource please note provider aggregated feature mean visible provider user combination organizationlevel setting grouplevel setting example organization enables google cluster group enables azure cluster group user able create google azure cluster instead organization provider disabled group azure cluster enabled group user able create azure cluster lastly organization google cluster enabled group provider disabled group user able create google cluster enabling provider section explains choose provider pw organization able access info persona step included page completed pw platform administrator user orgadmin role information please see group role navigate organization setting username organization next page click provider currently offered resource type listed provider default setting selected user access resource provider checkboxes provider locked default setting selected select custom setting make change enable provider click empty checkbox whichever resource want organization remove provider click blue checkboxes disable existing resource default item resource name collapsed expand click select deselect individual resource type category youre done click save provider setting change applied immediately info customizing default provider follow step change apply organization youre organization name see first navigate organization page', 'configuring instance type page explains choose instance type user access configure cluster resource tab feature useful example user need certain amount power work case limit instance lower cpu type info persona step included page completed pw platform administrator user orgadmin role information please see group role navigate organization setting username organization next page click provider provider navigate allowed instance type default setting selected user access instance type select custom setting make change customize instance type aws azure google resource use arrow csp open dropdown menu click instance type want add instance type select appear list dropdown menu youre done click save instance type change applied immediately', 'sidebarlabel managing group title import fatrash reacticonsfa group page explains major feature pw group use group platform assign user permission designate cost allocation manage cloud service provider share resource user info persona step included page completed user either orgadmin orggroups role information please see group role navigating group page navigate organization setting username organization next page click group creating group group tab click new group enter name group optionally enter description click create group new group listed group tab adding user group create group add new member page manage user group use dropdown menu select member organization click add member new user listed group member dropdown menu info group group cluster configuration setting recommend letting user know theyre multiple group group use cluster removing user group need remove member group click remove icon next user name click remove icon dialog box appear message removing user username group click cancel go back remove user remove user group click remove user user longer appear list group member dropdown menu info removing user removing user group affect permission group may use provision cloud resource account access platform affected assigning role add role click empty checkbox whichever permission want group remove role click blue checkboxes disable existing permission youre done click save role change applied immediately editing allocation info persona step included section completed pw platform administrator user orgadmin role group tab click edit cost allocation enter value allocation total field number measured u dollar group reach allocation total user longer able start cluster click save allocation youve set allocation total group accumulated cost appear allocation used column user see warning cluster group accumulated cost approaching allocation total', 'restarting user workspace page explains restart user workspace feature useful user experiencing error platform info persona step included page completed pw platform administrator user orgadmin role information please see group role navigate organization setting username organization next page click user user click user name user profile setting click restart workspace dialog box appear message sure want restart user workspace click restart workspace restarted youll see message user workspace killed info troubleshooting please contact u restarted workspace user experiencing error error arent resolved restarting', 'title sidebarlabel configuring default resource default resource page explains choose resource new user able access navigate resource tab feature useful youve configured cluster account want organization user configuration setting available choose default resource new user copy resource made whenever create new account default resource separate user affected work completed default resource change made configuration setting configuring default resource info persona step included page completed pw platform administrator user either orgadmin orgsettings role information please see group role navigate organization setting username organization setting click dropdown menu new user default resource see resource account dropdown menu click resource want add new user resource appear list dropdown menu youre done adding resource click save default resource change applied immediately', 'preparing aws page explains set aws account ready pw platform manage infrastructure cluster billing storage usage data info persona step included page completed cloud engineer organization aws account recommend creating new aws account pw platform allow keep existing aws account separate platform make easier manage billing usage data also ensure principle least privilege pw platform access resource need manage use aws organization create new account within organization otherwise make nonorganization account setting aws credential get started quickly create new iam user assign administratoraccess awsmanaged policy alternatively create policy listed aws policy attach policy iam user create access key pw platform requires use aws access key authenticate aws dont access key create one info security best practice pw platform immediately rotate secret access key entered system platform use rotated secret access key generate shortterm credential used pw platform service information aws key security best practice see faq aws website aws policy section includes policy youll need attach iam user create pw platform create policy iam console create aws cli entering json file listed policy pwinfraclusters policy used creating cluster json version 20121017 statement effect allow action ec2authorizesecuritygroupingress ec2describeinstances ec2createkeypair ec2createimage ec2copyimage ec2describesnapshots ec2describeplacementgroups ec2deletevolume ec2modifysnapshotattribute ec2createplacementgroup ec2describevolumes ec2describekeypairs ec2detachvolume fsxlisttagsforresource ec2importkeypair ec2createtags ec2registerimage ec2modifynetworkinterfaceattribute route53changeresourcerecordsets ec2deletenetworkinterface fsxuntagresource ec2runinstances ec2stopinstances ec2createvolume ec2createnetworkinterface ec2getpassworddata ec2describeimageattribute ec2describeinstancetypes ec2associateaddress ec2describesubnets ec2deletekeypair ec2attachvolume ec2disassociateaddress fsxdescribefilesystems ec2deregisterimage ec2deletesnapshot ec2describeinstanceattribute ec2describeregions ec2modifyimageattribute iampassrole ec2describenetworkinterfaces ec2createsecuritygroup ec2createsnapshot ec2modifyinstanceattribute fsxtagresource ec2describeinstancestatus ec2terminateinstances ec2detachnetworkinterface ec2deleteplacementgroup ec2describetags ec2describesecuritygroups fsxdeletefilesystem ec2describeimages ec2describevpcs ec2deletesecuritygroup fsxcreatefilesystem ec2attachnetworkinterface resource pwinfracreate policy used create base infrastructure json version 20121017 statement effect allow action ec2associatedhcpoptions ec2associateroutetable ec2attachinternetgateway ec2authorizesecuritygroupegress ec2authorizesecuritygroupingress ec2createdhcpoptions ec2createinternetgateway ec2createnatgateway ec2createroute ec2createroutetable ec2createsecuritygroup ec2createsubnet ec2createvpc ec2describeaccountattributes ec2describeaddresses ec2describeavailabilityzones ec2describedhcpoptions ec2describeinternetgateways ec2describenatgateways ec2describenetworkacls ec2describeroutetables ec2describesecuritygroups ec2describesubnets ec2describevpcattribute ec2describevpcclassiclink ec2describevpcclassiclinkdnssupport ec2describevpcs ec2modifyvpcattribute ec2revokesecuritygroupegress ec2createtags ec2allocateaddress iamaddroletoinstanceprofile iamcreateaccesskey iamcreateinstanceprofile iamcreaterole iamcreateuser iamgetinstanceprofile iamgetrole iamgetrolepolicy iamgetuser iamlistattachedrolepolicies iamlistrolepolicies iamputrolepolicy iamputuserpolicy iamtagrole iamtaguser iampassrole route53changetagsforresource route53createhostedzone route53getchange route53gethostedzone route53listresourcerecordsets route53listtagsforresource stsgetcalleridentity resource pwinfradelete policy used delete base infrastructure json version 20121017 statement effect allow action stsgetcalleridentity stsgetcalleridentity ec2describeaccountattributes ec2describeavailabilityzones ec2describeaddresses ec2describevpcs ec2deletetags iamgetuser iamgetrole iamuntagrole iamuntaguser ec2describevpcattribute iamlistrolepolicies iamlistaccesskeys iamgetuserpolicy ec2describevpcattribute iamgetrolepolicy iamlistattachedrolepolicies ec2describevpcclassiclink iamgetinstanceprofile iamgetrolepolicy ec2describevpcclassiclinkdnssupport iamgetrole ec2describeroutetables ec2describenetworkacls ec2describesecuritygroups ec2describeroutetables ec2describesubnets ec2describesubnets ec2describesubnets ec2describesubnets ec2describesubnets ec2describesubnets ec2describesubnets ec2describesubnets ec2describesubnets ec2describesecuritygroups ec2describesubnets ec2describeinternetgateways route53gethostedzone ec2describesubnets ec2describesubnets ec2describeroutetables ec2describeroutetables ec2describeroutetables ec2describeroutetables ec2describeroutetables ec2describeroutetables ec2describeroutetables ec2describeroutetables ec2describeroutetables ec2describenatgateways ec2describeroutetables ec2describeroutetables ec2describeroutetables ec2describeroutetables ec2describeroutetables route53listresourcerecordsets ec2describeroutetables route53listtagsforresource ec2describedhcpoptions ec2describevpcs stsgetcalleridentity stsgetcalleridentity ec2describeaccountattributes ec2disassociateroutetable ec2disassociateroutetable iamdeleteaccesskey iamdeleteuserpolicy iamremoverolefrominstanceprofile ec2disassociateroutetable ec2disassociateroutetable ec2disassociateroutetable ec2associatedhcpoptions ec2deleteroute iamdeleteinstanceprofile iamdeleterolepolicy ec2disassociateroutetable ec2disassociateroutetable ec2describeroutetables ec2describeroutetables ec2describeroutetables ec2disassociateroutetable ec2describeroutetables ec2describenetworkinterfaces ec2describeroutetables ec2describeroutetables ec2describeroutetables ec2disassociateroutetable ec2disassociateroutetable ec2disassociateroutetable ec2disassociateroutetable ec2deleteroute ec2describeroutetables ec2describeroutetables ec2describeroutetables iamlistgroupsforuser ec2deletedhcpoptions ec2describeroutetables ec2describeroutetables iamdeleteuser ec2describeroutetables ec2describenetworkinterfaces iamlistinstanceprofilesforrole ec2describeroutetables ec2describeroutetables iamlistrolepolicies ec2describenetworkinterfaces ec2describenetworkinterfaces ec2describenetworkinterfaces ec2describenetworkinterfaces ec2describenetworkinterfaces route53listresourcerecordsets iamdeleterole ec2deletesubnet ec2deletesubnet ec2deletesubnet ec2deletesubnet ec2deletesecuritygroup ec2deletesubnet ec2describeroutetables ec2deletenatgateway ec2describeroutetables ec2deletesubnet ec2deleteroutetable ec2describeroutetables route53getdnssec route53deletehostedzone ec2describenatgateways ec2describenatgateways ec2describenatgateways ec2describenatgateways ec2describenatgateways ec2describenetworkinterfaces ec2describenetworkinterfaces ec2describenetworkinterfaces ec2describenetworkinterfaces ec2describeaddresses ec2describenetworkinterfaces ec2describenetworkinterfaces ec2deletesubnet ec2deletesubnet ec2deletesubnet ec2deletesubnet ec2deletesubnet ec2deletesubnet ec2releaseaddress ec2detachinternetgateway ec2deleteinternetgateway ec2deletevpc resource pwbilling policy used access billing information json version 20121017 statement effect allow action s3getlifecycleconfiguration s3getbuckettagging s3putaccelerateconfiguration s3deleteobjectversion s3listbucketversions s3getbucketlogging s3createbucket s3listbucket s3getaccelerateconfiguration s3getbucketpolicy s3putencryptionconfiguration s3getobjectacl s3getencryptionconfiguration s3getbucketobjectlockconfiguration s3putbuckettagging s3getbucketrequestpayment s3putlifecycleconfiguration s3putbucketacl cur s3deleteobject s3deletebucket s3putbucketversioning s3putobjectacl s3getbucketpolicystatus s3getbucketwebsite s3putreplicationconfiguration s3getbucketversioning s3putbucketcors s3getbucketacl s3deletebucketpolicy s3getreplicationconfiguration s3putobject s3getobject s3putbucketwebsite s3listallmybuckets s3putbucketrequestpayment s3putbucketlogging s3getbucketcors s3putbucketpolicy s3putbucketobjectlockconfiguration s3getbucketlocation s3getobjectversion resource', 'preparing google page explains set google account ready pw platform manage infrastructure cluster billing storage usage data info persona step included page completed cloud engineer organization google account recommend creating new google project pw platform allow keep existing google project separate platform make easier manage billing usage data also ensure principle least privilege pw platform access resource need manage setting google credential get started quickly create new service account add owner role allow pw platform manage resource project want limit scope service account create custom role assign service account creating service account key pw platform us service account key authenticate google cloud create new service account key following step google documentation google permission section includes permission role youll need assign google service account create pw platform create custom role needed permission iam console pwbilling permission used provision access billing infrastructure also assign existing google iam role bigquery user service usage admin google service account json serviceusageoperationsget serviceusageservicesdisable serviceusageservicesenable serviceusageservicesget serviceusageserviceslist monitoringtimeserieslist serviceusageoperationscancel serviceusageoperationsdelete serviceusageoperationslist serviceusagequotasget serviceusagequotasupdate serviceusageservicesuse bigquerydatasetscreate bigquerydatasetsget bigqueryjobscreate bigquerytableslist resourcemanagerprojectsget bigquerybireservationsget bigquerycapacitycommitmentsget bigquerycapacitycommitmentslist bigqueryconfigget bigquerydatasetsgetiampolicy bigqueryjobslist bigquerymodelslist bigqueryreadsessionscreate bigqueryreadsessionsgetdata bigqueryreadsessionsupdate bigqueryreservationassignmentslist bigqueryreservationassignmentssearch bigqueryreservationsget bigqueryreservationslist bigqueryroutineslist bigquerysavedqueriesget bigquerysavedquerieslist bigquerytransfersget bigquerymigrationtranslationtranslate resourcemanagerprojectslist', 'preparing azure page explains set azure subscription ready pw platform manage infrastructure cluster billing storage usage data info persona step included page completed cloud engineer organization azure subscription recommend creating new azure subscription pw platform allow keep existing azure subscription separate platform make easier manage billing usage data also ensure principle least privilege pw platform access resource need manage learn azure subscription please see azure documentation setting azure credential get sarted quickly create new service principal add contributor role allow pw platform manage resource subscription want limit scope service principal create custom role assign service principal creating client secret pw platform us azure client secret authenticate azure create new client secret following step azure documentation azure access policy pwbilling assign following predefined role application order pw platform provision billing infrastructure access true cost data json contributor storage blob data contributor', 'import tab themetabs import tabitem themetabitem adding cloud account page explains add cloud service provider key pw organization allow provision infrastructure member organization use start cluster info persona step included page completed administrator organization navigate organization setting username organization next page click key key click new key select cloud service provider type dropdown menu next youll add cloud account key process look slightly different cloud service provider choose one youre using option aws us access key authentication youll need access key private access key add aws account pw platform dont access key create one azure us service principle authenticated client secret youll need client id tenant id client secret subscription id add azure account pw platform dont principal secret create one google us service account key authentication youll need json file key add google account pw platform dont service account key create one entering credential click create cloud key next page youll see message security credential added new key listed key tab', 'creating billing infrastructure page explains provision billing infrastructure pw account allow pw access process display billing data cost dashboard info persona step included page completed administrator organization navigate organization setting username organization next page click infrastructure infrastructure click new infrastructure select saved key key dropdown menu dont saved key add one enter name infrastructure please note text must lowercase alphanumeric select billing infrastructure click create infrastructure next youll configure infrastructure process look slightly different cloud service provider please see configuring billing infrastructure information configuring infrastructure click save billing youll see automated message infrastructure updated click deploy infrastructure provisioning log display progress provisioning deployment process take five minute click infrastructure tab new infrastructure configuration listed info note currently configure one billing infrastructure per organization regardless csp region youre using', 'configuring billing infrastructure page explains configure billing infrastructure according cspspecific parameter info persona step included page completed administrator organization functionality change infrastructure configuration click definition tab button tab following function save billing save current configuration parameter whenever change field infrastructure configuration form use button ensure change arent lost delete configuration deletes current billing infrastructure configuration use button infrastructure configuration appear list configured infrastructure deploy infrastructure provision billing infrastructure destroy infrastructure deprovisions billing infrastructure force unlock release billing infrastructure locked state infrastructure redeployed button useful infrastructure provisioner fails deployment process information see troubleshooting see code version configuration setting click  json tab change infrastructure name click property tab info importing also imported option work csps use option already provisioned configured billing infrastructure csp account want pw platform access process billing data pw platform manage imported billing infrastructure system assumes infrastructure already provisioned ready use imported billing infrastructure provisioned destroyed info deletion want delete configuration list infrastructure first need deprovision infrastructure configuration page dont infrastructure still exist csp account aws billing infrastructure pw provisioned aws billing infrastructure select region want billing infrastructure deployed information region see aws documentation imported aws billing infrastructure toggle imported button yes enter s3 bucket name select region bucket provisioned info note pw platform manages billing data attaching tag provisioned cloud resource enable necessary tag aws billing data youll need activate custom tag aws account please see aws documentation required permission complete step listed log aws management console navigate billing page click cost allocation tag tab userdefined allocation tag activate tag date groupid name organizationid pool project session sessionid user azure billing infrastructure pw provisioned azure billing infrastructure select region want billing infrastructure deployed information region see azure documentation toggle cost management export button yes yet registered microsoftcostmanagementexports provider azure subscription toggle button yes already registered microsoftcostmanagementexports provider provisioner fail azure deny new registration attempt detail provider please see azure documentation imported azure billing infrastructure toggle imported button yes enter storage account name blob container name youve configured store azure billing data google billing infrastructure pw provisioned google billing infrastructure select region want billing infrastructure deployed information region see google documentation project field enter project id project want billing infrastructure deployed google credential key need certain permission enabled project id information required permission see preparing google info note time writing page google automated option export billing data provisioned bigquery dataset task must performed manually need role billing exporter billing account complete step listed log google cloud console navigate billing page click billing export select detailed usage cost project choose project provisioned bigquery dataset dataset choose bigquery dataset provisioned dataset name format yourogranizationbilling click save information please see step 5 google guide setting cloud billing imported google billing infrastructure toggle imported button yes enter bigquery dataset name project id dataset provisioned troubleshooting clicking deploy infrastructure wont able edit configuration setting infrastructure built field button grayed except destroy infrastructure force unlock provisioning log display error red retry provisioning process clicking destroy infrastructure delete provisioned element click deploy infrastructure retry provisioning process error persist click force unlock allow edit configuration setting delete infrastructure delete configuration caution warning please monitor provisioning log wait provisioning process finish attempting force unlock retry deploy infrastructure destroy infrastructure force unlock deploy infrastructure provisioning process still running fail cause unwanted issue error persist trying redeploy reconfigure infrastructure please contact u', 'configuring base infrastructure page explains configure base infrastructure according cspspecific parameter info persona step included page completed administrator organization functionality change infrastructure configuration click definition tab button tab following function save base save current configuration parameter whenever change field infrastructure configuration form use button ensure change arent lost delete configuration deletes current base infrastructure configuration use button infrastructure configuration appear list configured infrastructure deploy infrastructure provision base infrastructure destroy infrastructure deprovisions base infrastructure force unlock release base infrastructure locked state infrastructure redeployed button useful infrastructure provisioner fails deployment process information see troubleshooting see code version configuration setting click  json tab change infrastructure name click property tab info deletion want delete configuration list infrastructure first need deprovision infrastructure configuration page dont infrastructure still exist csp account aws infrastructure select region want infrastructure deployed information region see aws documentation azure infrastructure select region want infrastructure deployed information region see azure documentation vm image resource group id enter resource id resource group want infrastructure associated dont resource group create one find resource id azure portal navigating home payasyougo resource group groupname property resource id vm image resource group name enter name resource group want infrastructure associated nat ip availability zone enter availability zone want infrastructure ip address deployed suggest entering zoneredundant field information availability zone see azure documentation google infrastructure enter project id project want infrastructure deployed dont project please see preparing google select region availability zone want infrastructure deployed information region availability zone see google documentation troubleshooting clicking deploy infrastructure wont able edit configuration setting infrastructure built field button grayed except destroy infrastructure force unlock provisioning log display error red retry provisioning process clicking destroy infrastructure delete provisioned element click deploy infrastructure retry provisioning process error persist click force unlock allow edit configuration setting delete infrastructure delete configuration caution warning please monitor provisioning log wait provisioning process finish attempting force unlock retry deploy infrastructure destroy infrastructure force unlock deploy infrastructure provisioning process still running fail cause unwanted issue error persist trying redeploy reconfigure infrastructure please contact u', 'creating base infrastructure page explains provision base infrastructure pw account allow member organization work cluster info persona step included page completed administrator organization navigate organization setting username organization next page click infrastructure infrastructure click new infrastructure select saved key key dropdown menu dont saved key add one enter name infrastructure please note text must lowercase select base infrastructure click create infrastructure next youll configure infrastructure process look slightly different cloud service provider please see configuring base infrastructure information configuring infrastructure click update infrastructure configuration youll see automated message infrastructure updated click deploy infrastructure provisioning log display progress provisioning deployment process take five minute click infrastructure tab new infrastructure configuration listed info region currently configure one infrastructure per region regardless csp youre using youd like add second infrastructure csp different region youll need deploy another infrastructure']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def initialize_vectorizer(docs):\n",
    "    \"\"\"\n",
    "    Initializes a TF-IDF vectorizer model on inputted documents.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, stop_words='english')\n",
    "    tokenized_corpus = list(docs.values())\n",
    "    corpus = [' '.join(tokens) for tokens in tokenized_corpus]\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "vectorizer, tfidf_matrix = initialize_vectorizer(cp_doc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, docs, vectorizer, tfidf_matrix):\n",
    "    \"\"\"\n",
    "    Runs a semantic search with a query on inputted docs.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The query string\n",
    "        docs (list[str]): List of document names corresponding to the tfidf_matrix rows\n",
    "        vectorizer (TfidfVectorizer): The initialized TF-IDF vectorizer\n",
    "        tfidf_matrix (scipy.sparse.csr_matrix): The TF-IDF matrix\n",
    "\n",
    "    Returns:\n",
    "        results (list): List of tuples containing similar documents and their similarity scores\n",
    "    \"\"\"\n",
    "    corrected_query = cleaner.correct_spelling(query)\n",
    "    cp_query = clean_and_preproc_data(corrected_query)\n",
    "    query_vector = vectorizer.transform([\" \".join(cp_query)])\n",
    "    \n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix)\n",
    "    sorted_indexes = np.argsort(similarity_scores, axis=1)[0][::-1]\n",
    "\n",
    "    filenames = list(docs.keys())\n",
    "    similar_docs = [(filenames[i], similarity_scores[0][i]) for i in sorted_indexes]\n",
    "\n",
    "    return similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_files(query, docs, vectorizer, tfidf_matrix, top_k=5, include_score=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Gets the top 'k' relevant files from an inputted query. Defaults to top\n",
    "    5 most relevant files.\n",
    "\n",
    "    Parameters:\n",
    "        query (str) : question to search PW documentation for\n",
    "        top_k (int) : top 'k' most relevant files to return (default: 5)\n",
    "        include_score (bool) : if True, includes similarity score of file\n",
    "        verbose (bool) : if True, prints files in addition to returning\n",
    "    \n",
    "    Returns:\n",
    "        rel_files (list) : top 'k' most relevant files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        similar_docs = semantic_search(query, docs, vectorizer, tfidf_matrix)\n",
    "    except TypeError:\n",
    "        print(\"Your query does not match anything in our system.\")\n",
    "        return []\n",
    "\n",
    "    if include_score:\n",
    "        rel_files = similar_docs[:top_k]\n",
    "        if verbose:\n",
    "            print(f\"Top {top_k} most relevant files to your query with similarity scores included:\\n\")\n",
    "            for i, (file, sim_score) in enumerate(rel_files):\n",
    "                print(f\"{i + 1}. {file}: {sim_score}\")\n",
    "        return rel_files\n",
    "    else:\n",
    "        rel_files = [filename for filename, _ in similar_docs[:top_k]]\n",
    "        if verbose:\n",
    "            print(f\"Top {top_k} most relevant files to your query:\\n\")\n",
    "            for i, file in enumerate(rel_files):\n",
    "                print(f\"{i + 1}. {file}\")\n",
    "    return rel_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most relevant files to your query with similarity scores included:\n",
      "\n",
      "1. creating-storage.md: 0.143965402274469\n",
      "2. transferring-data-aws.md: 0.12212988441870776\n",
      "3. starting-stopping-clusters.md: 0.09006055848789304\n",
      "4. configuring-storage.md: 0.0816456811654708\n",
      "5. about-the-platform.md: 0.07299028610239347\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the best way to authenticate into an S3 bucket?\"\n",
    "\n",
    "get_relevant_files(query, cp_doc_data, vectorizer, tfidf_matrix, include_score=True, verbose=True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
