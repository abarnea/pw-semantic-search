{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search Model First Draft\n",
    "\n",
    "This Jupyter notebook is meant to serve as an introduction to reading Github `.md` documentation and analyzing it..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Documentation Data Reading and Pre-Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Reading and Storing the Documentation Data\n",
    "\n",
    "In this section, we'll read the markdown `.md` file data, collect it, and store it for processing. We can do this by reading through all of the `.md` files in a directory and reading them into plain text format, then storing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def read_md_file(filepath: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads a markdown file and processes it into a string of plain text.\n",
    "\n",
    "    Parameters:\n",
    "        filepath (str) : the filepath of the markdown file to read\n",
    "    \n",
    "    Returns:\n",
    "        text (str) : the plain text from the inputted markdown file\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        content = f.read()\n",
    "        html = markdown.markdown(content)\n",
    "        text = ''.join(BeautifulSoup(html).findAll(text=True))\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def collect_doc_data(directory: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Scans through a directory and collects the documentation data from all\n",
    "    '.md' files into a list.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str) : directory to scan through\n",
    "    \n",
    "    Returns\n",
    "        docs_data (list[str]) : documentation data from `.md` files\n",
    "    \"\"\"\n",
    "    doc_data = {}\n",
    "    for dirpath, _, filenames in os.walk(directory):\n",
    "        for file in filenames:\n",
    "            if file.endswith('.md'):\n",
    "                file_path = os.path.join(dirpath, file)\n",
    "                text = read_md_file(file_path)\n",
    "                doc_data[file] = text\n",
    "    \n",
    "    return doc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/lp_w7fr92mj40y7_28ynvst40000gn/T/ipykernel_46160/3992071256.py:17: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  text = ''.join(BeautifulSoup(html).findAll(text=True))\n"
     ]
    }
   ],
   "source": [
    "doc_data = collect_doc_data(\"docs/docs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Cleaning the Documentation Data\n",
    "\n",
    "In this section, we'll take our collected and stored documentation data from Step 1 and clean it up so we can use it. This could include removing HTML tags, removing punctuation and special characters, removing extra whitespaces from the text, making all of our text lowercase for semantic searching, and catching any mispellings in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def _remove_whitespace(input_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes whitespace from an input string.\n",
    "    \"\"\"\n",
    "    return ' '.join(input_str.split())\n",
    "\n",
    "def _lower_str(input_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Lowers the input string to lower case\n",
    "    \"\"\"\n",
    "    return input_str.lower()\n",
    "\n",
    "def _remove_punct_and_special_chars(input_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes punctuation and special characters using Regex\n",
    "    \"\"\"\n",
    "    pattern = r'[^\\w\\s]'\n",
    "    return re.sub(pattern, '', input_str)\n",
    "\n",
    "\n",
    "def _filter_sidebar_pos(input_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes sidebar positioning.\n",
    "    \"\"\"\n",
    "    pattern = r\"sidebar_position(?: \\d+)? \"\n",
    "    return re.sub(pattern, \"\", input_str)\n",
    "\n",
    "def clean_str(input_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Applies data cleaning measures to input string, including removing whitespaces,\n",
    "    lowercasing the string, removing punctuations and special characters, and\n",
    "    filtering sidebar positioning from .md files.\n",
    "\n",
    "    Parameters:\n",
    "        input_str (str) : inputted string to be cleaned\n",
    "    \n",
    "    Returns:\n",
    "        cleaned_str (str) : cleaned string\n",
    "    \"\"\"\n",
    "    cleaning_funcs = [_remove_whitespace, _lower_str,\n",
    "                        _remove_punct_and_special_chars, _filter_sidebar_pos]\n",
    "    \n",
    "    cleaned_str = input_str\n",
    "    for func in cleaning_funcs:\n",
    "        cleaned_str = func(cleaned_str)\n",
    "\n",
    "    return cleaned_str\n",
    "\n",
    "def clean_doc_data(doc_data: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Clean the documentation data by removing HTML tags, removing punctuation\n",
    "    and special characters, removing extra whitespaces, making everything\n",
    "    lowercase, and catching mispelled words.\n",
    "\n",
    "    Parameters:\n",
    "        doc_data (list[str]) : the collected and read `.md` data\n",
    "    \n",
    "    Returns:\n",
    "        cleaned_doc_data (list[str]) : the cleaned documentation data\n",
    "    \"\"\"\n",
    "    return {file : clean_str(content) for file, content in doc_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_doc_data = clean_doc_data(doc_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Pre-processing the Documentation Data\n",
    "\n",
    "In this section, we'll take our cleaned documentation data from Step 2 and pre-process it by tokenization, stemming lemmatization, and stop-word removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def tokenize_str(input_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Tokenize an input string.\n",
    "    \"\"\"\n",
    "    return word_tokenize(input_str)\n",
    "\n",
    "def _remove_stopwords(tokens: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Remove stop-words from a list of tokens.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def _stem_tokens(tokens: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Stems tokens.\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def _lemmatize_tokens(tokens: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Lemmatizes tokens.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def _clean_tokens(tokens: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Clean tokens again.\n",
    "    \"\"\"\n",
    "    return [re.sub(r'[^a-zA-Z0-9]', '', token) for token in tokens if token]\n",
    "\n",
    "def preprocess_str(cleaned_str: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Helper function to preprocess an inputted string via tokenization, stemming or \n",
    "    lemmatizing, and stop-word removal.\n",
    "\n",
    "    Parameters:\n",
    "        cleaned_str (str) : a pre-cleaned string\n",
    "    \n",
    "    Returns:\n",
    "        preproc_tokens (list[str]) : preprocessed tokens of a string\n",
    "    \"\"\"\n",
    "    tokens = tokenize_str(cleaned_str)\n",
    "\n",
    "    preproc_funcs = [_remove_stopwords, _lemmatize_tokens, _clean_tokens] \n",
    "\n",
    "    preproc_tokens = tokens\n",
    "    for func in preproc_funcs:\n",
    "        preproc_tokens = func(preproc_tokens)\n",
    "    \n",
    "    return preproc_tokens\n",
    "\n",
    "def preprocess_doc_data(cleaned_doc_data: list[str]) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Preprocesses the full documentation data set by tokenizing each string entry\n",
    "    in the inputted documentation data, then removing stop words and lemmatizing\n",
    "    the tokens via the WordNetLemmatizer algorithm.\n",
    "\n",
    "    Parameters:\n",
    "        cleaned_doc_data (list[str]) : cleaned documentation data\n",
    "    \n",
    "    Returns:\n",
    "        preproc_doc_data (list[list[str]]) : full pre-processed documentation data\n",
    "    \"\"\"\n",
    "    return {file : preprocess_str(content) for file, content in cleaned_doc_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_docs = preprocess_doc_data(cleaned_doc_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Implementing Semantic Search with Word2Vec\n",
    "\n",
    "Now, we can use `Gensim` to implement the semantic searching of the cleaned and pre-processed documentation data with its Word2Vec algorithm. This basically maps words and phrases to dense vector representations in a high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.downloader\n",
    "\n",
    "# pretrained_model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "corpus = list(preproc_docs.values())\n",
    "model = Word2Vec(corpus, vector_size=500, window=5, min_count=5, workers=4)\n",
    "# model.build_vocab_from_freq(pretrained_model.key_to_index, corpus_count=len(corpus), update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = {}\n",
    "\n",
    "for filename, tokens in preproc_docs.items():\n",
    "    embeddings = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if embeddings:\n",
    "        document_embeddings[filename] = gensim.matutils.unitvec(np.mean(embeddings, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query_str: str):\n",
    "    \"\"\"\n",
    "    Performs a similarity search on the inputted query against the given Word2Vec model.\n",
    "\n",
    "    Parameters:\n",
    "        query_str (str) : user query\n",
    "    \n",
    "    Returns:\n",
    "        similar_docs (list[str]) : most similar docs in order\n",
    "    \"\"\"\n",
    "    query_tokens = preprocess_str(clean_str(query_str))\n",
    "    average_vec_rep = [model.wv[token] for token in query_tokens if token in model.wv]\n",
    "    query_embedding = gensim.matutils.unitvec(np.mean(average_vec_rep, axis=0))\n",
    "\n",
    "    similar_docs = []\n",
    "    for filename, doc_embedding in document_embeddings.items():\n",
    "        similarity_score = np.dot(doc_embedding, query_embedding)\n",
    "        similar_docs.append((filename, similarity_score))\n",
    "    \n",
    "    similar_docs = sorted(similar_docs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_files(query, top_k=5, include_score=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Gets the top 'k' relevant files from an inputted query. Defaults to top\n",
    "    5 most relevant files.\n",
    "\n",
    "    Parameters:\n",
    "        query (str) : question to search PW documentation for\n",
    "        top_k (int) : top 'k' most relevant files to return (default: 5)\n",
    "        include_score (bool) : if True, includes similarity score of file\n",
    "        verbose (bool) : if True, prints files in addition to returning\n",
    "    \n",
    "    Returns:\n",
    "        rel_files (list) : top 'k' most relevant files\n",
    "    \"\"\"\n",
    "    similar_docs = run_query(query)\n",
    "\n",
    "    if include_score:\n",
    "        rel_files = similar_docs[:top_k]\n",
    "        if verbose:\n",
    "            print(f\"Top {top_k} most relevant files to your query with similarity scores included:\\n\")\n",
    "            for i, (file, sim_score) in enumerate(rel_files):\n",
    "                print(f\"{i + 1}. {file}: {sim_score}\")\n",
    "        return rel_files\n",
    "    else:\n",
    "        rel_files = [filename for filename, _ in similar_docs[:top_k]]\n",
    "        if verbose:\n",
    "            print(f\"Top {top_k} most relevant files to your query:\\n\")\n",
    "            for i, file in enumerate(rel_files):\n",
    "                print(f\"{i + 1}. {file}\")\n",
    "    return rel_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most relevant files to your query with similarity scores included:\n",
      "\n",
      "1. configuring-storage.md: 0.9999051094055176\n",
      "2. configuring-clusters.md: 0.99989914894104\n",
      "3. navigating-the-admin-panel.md: 0.9998986124992371\n",
      "4. logging-in-controller.md: 0.9998984932899475\n",
      "5. configuring-instances.md: 0.9998974800109863\n"
     ]
    }
   ],
   "source": [
    "query = \"Where do I look for a question about monitoring my costs?\"\n",
    "\n",
    "get_relevant_files(query, include_score=True, verbose=True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
